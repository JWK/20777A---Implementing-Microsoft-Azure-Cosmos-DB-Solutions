# Module 6: Implementing Server-Side Operations

- [Module 6: Implementing Server-Side Operations](#module-6-implementing-server-side-operations)
  - [Lab: Querying and analyzing big data with Cosmos DB](#lab-querying-and-analyzing-big-data-with-cosmos-db)
  - [Exercise 1: Extending product search capabilities](#exercise-1-extending-product-search-capabilities)
    - [Prepare the Environment](#prepare-the-environment)
    - [Task 1: Create an Azure Search Service for the Cosmos DB database](#task-1-create-an-azure-search-service-for-the-cosmos-db-database)
    - [Task 2: Index the Data in the Cosmos DB Database](#task-2-index-the-data-in-the-cosmos-db-database)
    - [Task 3: Update the Adventure-Works app to use Azure Search](#task-3-update-the-adventure-works-app-to-use-azure-search)
    - [Task 4: Use the Suggester in the Search Field](#task-4-use-the-suggester-in-the-search-field)
  - [Exercise 2: Generating Month-end Order Summaries](#exercise-2-generating-month-end-order-summaries)
    - [Task 1: Create an HDInsight Cluster for Running Spark](#task-1-create-an-hdinsight-cluster-for-running-spark)
    - [Task 2: Upload Sample Orders to the Cosmos DB Database](#task-2-upload-sample-orders-to-the-cosmos-db-database)
    - [Task 3: Install and Configure PuTTY](#task-3-install-and-configure-putty)
    - [Task 4: Perform Interactive Queries Against Cosmos DB Using Spark](#task-4-perform-interactive-queries-against-cosmos-db-using-spark)
    - [Task 5: Perform Batch Operations by Using a Jupyter Notebook](#task-5-perform-batch-operations-by-using-a-jupyter-notebook)
  - [Exercise 3: Visualizing Sales Data](#exercise-3-visualizing-sales-data)
    - [Task 1: Create an Azure Databricks Service and Cluster](#task-1-create-an-azure-databricks-service-and-cluster)
    - [Task 2: Create a Library and PyDocumentDB Notebook](#task-2-create-a-library-and-pydocumentdb-notebook)
    - [Task 3: Write Python Code to Retrieve and Display the Data](#task-3-write-python-code-to-retrieve-and-display-the-data)
    - [Task 4: Create a Dashboard](#task-4-create-a-dashboard)
    - [Task 5: Cleanup the lab environment](#task-5-cleanup-the-lab-environment)

## Lab: Querying and analyzing big data with Cosmos DB

## Exercise 1: Extending product search capabilities

### Prepare the Environment

1. Ensure that the **MT17B-WS2016-NAT** and **20777A-LON-DEV** virtual machines are running, and then log on to **20777A-LON-DEV** as **LON-DEV\\Administrator** with the password **Pa55w.rd**.
2. In File Explorer, navgiate to **E:\\Labfiles\\Lab06\\Starter**, right-click **full-cosmos-setup.ps1.txt**, and then click **Edit**.
3. In Notepad, on line 2, edit **20777a-mod6-sql-\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example **20777a-mod6-sql-pjs14**.
4. On line 3, edit **20777a-mod6-\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example **20777a-mod6-pjs14**.
5. On line 4, edit **20777blobmod6\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example, **20777blobmod6pjs14**.
6. On line 5, set the **resourceGroupLocation** variable to the name of your nearest location.
7. On the **File** menu, click **Save**, and then close Notepad.
8. In File Explorer, right-click **Setup.cmd**, and then click **Run as administrator**.
9. If any **Security warning** messages appear, type **R**, and then press Enter.
10. When prompted enter your Azure credentials.
11. When the script completes, press any key to close the command window.

### Task 1: Create an Azure Search Service for the Cosmos DB database

1. In Internet Explorer, in the Azure portal, in the left panel, click **+ Create a resource**.
2. On the **New** blade, in the Search box, type **Azure Search**, and then press Enter.
3. On the **Everything** blade, click **Azure Search**, and then click **Create**.
4. On the **New Search Service** blade, in the **URL** box, type **20777a-search-\<*your name\>-\<the day*\>**, for example, **20777a-search-john-31**.
5. In the **Resource group** list, click **20777_Mod06**.
6. In the **Location** box, select the same location as your Cosmos DB account, or the closest physical location if the location you selected for your Cosmos DB account is not available.
7. Click **Pricing tier**.
8. On the **Choose your pricing tier** blade, click **Free**, and then click **Select**.
9. On the **New Search Service** blade, click **Create**, and wait for the search account to be created before proceeding.
10. In the Azure portal, in the left panel, click **All resources**, and then click **20777a-search-\<*your name\>-\<the day*\>**.
11. On the **20777a-search-\<*your name\>-\<the day*\>** blade, under **Settings**, click **Keys**, and then click **Manage query keys**.
12. Make a note of the  key for the **\<empty\>** value.

### Task 2: Index the Data in the Cosmos DB Database

1. In the left panel, click **All resources**, and then click **20777a-mod6-sql-\<*your initials and day*\>**.
2. On the **20777a-mod6-sql-\<*your initials and day*\>** blade, click **Data Explorer**.
3. In the **SQL API** pane, expand **Adventure-Works** , expand **Data**, and then click **Scale & Settings**.
4. On the **Scale & Settings** tab, modify the **Indexing Policy** to include the excluded paths shown below, and then click **Save**. This policy prevents the **description**, **documentation**, and **images** fields from being indexed. The **description** field is free text and can be quite large; you will configure Azure Search to index this field more efficiently. The **documentation** and **images** fields are references to items in blob storage, and indexing these fields is not worthwhile.

    ```JSON
    {
        "indexingMode": "consistent",
        "automatic": true,
        "includedPaths": [
            {
                "path": "/*",
                "indexes": [
                    {
                        "kind": "Range",
                        "dataType": "Number",
                        "precision": -1
                    },
                    {
                        "kind": "Range",
                        "dataType": "String",
                        "precision": -1
                    },
                    {
                        "kind": "Spatial",
                        "dataType": "Point"
                    }
                ]
            }
        ],
        "excludedPaths": [
            {
                "path": "/description/*"
            },
            {
                "path": "/documentation/*"
            },
            {
                "path": "/images/*"
            },
            {
                "path": "/\"_etag\"/?"
            }
        ]
    }
    ```
5. In the left panel, click **All resources**, and then click **20777a-search-\<*your name\>-\<the day*\>**.
6. On the **20777a-search-\<*your name\>-\<the day*\>** blade, click **Import Data**.
7. On the **Import data** blade, on the **Connect to your data** tab, in the **Data Source** list, click **Cosmos DB**.
8. In the **Name** box, type **productdata**.
9. In the **Cosmos DB acount** box, click **Choose an existing connection**, and then click **20777a-mod6-sql-\<*your initials and day*\>**.
10. In the **Database** drop-down list, click **Adventure-Works**.
11. In the **Collection** drop-down list, click **Data**, and then click **Next: Add cognitive search (Optional)**.
12. On the **Add cognitive search (Optional)** tab, click **Skip to: Customize target index**.
13. On the **Customize target index** tab, in the table, right-click the **ttl** row, and then click **Delete**.
14. In the **RETRIEVABLE** column, directly under the heading, select the check box (this will select all the rows for this column).
15. In the **FILTERABLE** column, select the following check boxes:
    - **id**
    - **partitionkey**
    - **doctype**
    - **productcategory**
    - **productname**
    - **productnumber**
    - **model**
    - **description**

16. In the **SEARCHABLE** column, select the following check boxes:
    - **productcategory**
    - **productname**
    - **productnumber**
    - **model**
    - **description**
17. In the **Suggester name** box, type **suggest**.
18. In the **Search mode** drop-down list, click **analyzingInfixMatching**.
19. Select the **Suggester** check box, and then click **Next: Create an indexer**.
20. On the **Create an Indexer** blade, in the **Name** box, type **productsindexer**.
21. Under **Schedule**, click **Hourly**, and then click **Submit**.
22. On the **20777a-search-\<*your name\>-\<the day*\>** blade, on the **Indexers (1)** tab, click the **productsindexer** index.
23. On the **productsindexer** blade, click **Run**.
24. In the **Run indexer** dialog box, click **Yes**.
25. Wait for a minute, and then close the **productsindexer** blade.
26. On the **20777a-search-\<*your name\>-\<the day*\>** blade, click the **productsindexer** index.
27. On the **productsindexer** blade, verify that indexing was completed successfully.
28. Close the **productsindexer** blade.
29. On the **20777a-search-\<*your name\>-\<the day*\>** blade, click **Search explorer**.
30. On the **Search explorer** blade, click **Search**. Verify that the search returns a list of documents.
31. In the **Query string** box, type **search=bike**, and then click **Search**. Verify that a list of documents appear that include the text **bike** in one of the searchable fields.
32. In the **Query string** box, type **search=natural**, and then click **Search**. This time only two documents should be retrieved. Both documents include the word **natural** in the **description** field.

### Task 3: Update the Adventure-Works app to use Azure Search

1. On the Start menu, click **Visual Studio 2017**.
2. On the **File** menu, point to **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab06\\Starter\\Exercise 1\\Adventure-Works**, and then double-click **Adventure-Works.sln**.
4. If the **Security Warning for MigrateProductData** dialog box appears, clear the **Ask me for every project in this solution** check box, and then click **OK**.
5. In Solution Explorer, click **Repository.cs**. The **Repository** implements the functionality required by the app to retrieve and maintain data in the Cosmos DB database. You will extend this class to incorporate Azure Search capabilities.
6. Find the **// TODO: Add namespaces containing Search functionality** comment, and add the following using directives:

    ```CSharp
    using Microsoft.Azure.Search;
    using Microsoft.Azure.Search.Models;
    ```

7. In the **Repository** class, after the **// TODO: Define fields to support connecting to the Azure Search Service** comment, add the following statements. This code defines the variables that the repository will use to connect to the search service. The **SearchIndexClient** type provides the methods that an app can use to send search requests and retrieve the results.

    ```CSharp
    private static string searchService;
    private static string queryKey;
    private static string searchIndex;
    private static string suggesterName;
    private static SearchIndexClient searchClient;
    ```

8. In the **Initialize** method, after the **// TODO: Initialize the connection to the Azure Search Service** comment, add the following statements. These statements read the parameters required to connect to the Azure Search Service from the web.config file. You will provide values for these parameters when you configure the application later.

    ```CSharp
    searchService = ConfigurationManager.AppSettings["SearchService"];
    queryKey = ConfigurationManager.AppSettings["SearchServiceQueryKey"];
    searchIndex = ConfigurationManager.AppSettings["SearchIndex"];
    suggesterName = ConfigurationManager.AppSettings["Suggester"];
    searchClient = new SearchIndexClient(searchService, searchIndex, new SearchCredentials(queryKey));
    ```

9. Find the **SearchForItemsAsync** method shown below. You will complete this method to use the Azure Search Service to find all documents in the Cosmos DB database that match a specified search term passed in as the **SearchString** parameter. The **advanced** parameter indicates whether the string in the **SearchString** parameter should be treated as a literal value, or it contains an Azure Search expression.

    ```CSharp
    // TODO: Use Azure Search to find items that match the specified search string
    public static async Task<IEnumerable<T>> SearchForItemsAsync(string searchString, bool advanced)
    {
        // TODO: If this is not an advanced query, enclose the search string in quotes to ensure that Azure Search treats it as a literal rather than a query expression

        // TODO:  Specify that the search should only return the id and partitionkey fields of the document from the index

        // TODO: Find the ids of all documents that match the search string

        // TODO: Iterate through the results, and construct a list of matching documents from the Cosmos DB database
        List<T> docs = new List<T>();
        foreach (var result in searchResults.Results)
        {
            // TODO: Construct the document URI for the document

            // TODO: Extract the partition key from the index

            // TODO: Fetch the document and add it to the list

        }

        // TODO: Return the list of documents identified by the search

    }
    ```

10. After the **// TODO: If this is not an advanced query, enclose the search string in quotes to ensure that Azure Search treats it as a literal rather than a query expression** comment, add the following block of code. If the **advanced** parameter is false, these statements add quotes to search string to ensure that it is handled as a literal value by the Azure Search Service.

    ```CSharp
    if (!advanced)
    {
        searchString = $"\"{searchString}\"";
    }
    ```

11. After the **// TODO: Specify that the search should only return the id and partitionkey fields of the document from the index** comment, add the following statements. The purpose of the Azure Search Service index is to quickly locate the id of matching documents in the Cosmos DB database. When you retrieve these documents from the database you will also require the partition key.

    ```CSharp
    var searchParams = new SearchParameters
    {
        Select = new [] { "id", "partitionkey" }
    };
    ```

12. After the **// TODO: Find the ids of all documents that match the search string** comment, add the following code that sends the search request to the Azure Search Service:

    ```CSharp
    var searchResults = await searchClient.Documents.SearchAsync<dynamic>(searchString, searchParams);
    ```

13. In the **foreach** loop that iterates through the results of the search request, after the **// TODO: Construct the document URI for the document** comment, add the following statements. This code uses the id returned by the Azure Search Service to build the URI of the corresponding document in the Cosmos DB database.

    ```CSharp
    string docID = result.Document.id;
    Uri docUri = UriFactory.CreateDocumentUri(database, collection, docID);
    ```

14. After the **// TODO: Extract the partition key from the index** comment, add the following statements. The **RequestOptions** object will be passed to a Cosmos DB request that fetches the document.

    ```CSharp
    string partitionkey = result.Document.partitionkey;
    var options = new RequestOptions
    {
        PartitionKey = new PartitionKey(partitionkey)
    };
    ```

15. After the **// TODO: Fetch the document and add it to the list** comment, add the following statements. Racall that the **ReadDocumentAsync** method provides the fastest way to access data in a Cosmos DB database.

    ```CSharp
    var response = await client.ReadDocumentAsync<T>(docUri, options);
    docs.Add(response.Document);
    ```

16. After the **// TODO: Return the list of documents identified by the search** comment, replace that code that throws the **NotImplementedException** with the following statement:

    ```CSharp
    return docs;
    ```

17. In Solution Explorer, expand **Models**, and then click **ViewModels.cs**.
18. On the **ViewModels.cs** tab, in the **ProductViewModel** class, after the **// TODO: Add the "AdvancedSearch" field to the view model, to record whether the user has selected a simple text or expression-based search** comment, add the following property:

    ```CSharp
    public bool AdvancedSearch { get; set; }
    ```

19. In Solution Explorer, expand **Controllers**, and then click **ProductsController.cs**.
20. On the **ProductsController.cs** tab, find the **SearchProductsAsync** method. You will complete this method to search for products using the repository.
21. After the **// TODO: Use the repository to find products that match the specified search string in the Product View Model** comment, add the following statement:

    ```CSharp
    IEnumerable<Product> products = await Repository<Product>.SearchForItemsAsync(productViewModel.SearchString, productViewModel.AdvancedSearch);
    ```

22. After the **// TODO: Construct a new view model containing the results and display it** comment, replace the statement that throws the **NotImplementException** with the following code. This statement constructs a new view model containing the results of the search, and sends the view model back to the view for display.

    ```CSharp
    return View("FindProducts", new ProductViewModel
    {
        Products = products,
        SelectableCategories = Session["selectableCategories"] as ProductCategoryViewModel ?? await InitializeCategoriesAsync(),
        SelectableSubcategories = Session["selectableSubcategories"] as ProductSubcategoryViewModel ?? await InitializeSubCategoriesAsync()
    });
    ```

23. In Solution Explorer, expand **Views**, expand **Products**, and then click **FindProducts.cshtml**.
24. On the **FindProducts.cshtml** tab, locate the **\<!-- TODO: Adjust the form to enable the user to specify a simple or advanced search -->** comment. Note that this form calls the **SearchProducts** action (which runs the **SearchProductsAsync** method) in the **ProductsController**.
25. Replace the **@Html.TextBoxFor** statement with the following markup. This code adds the search string entered by the user on the form, together with a boolean indicating whether the user selected the **Advanced** check box, to the view model which is passed to the **SearchProductsAsync** method.

    ```HTML
    <table class="table">
        <tr>
            <td>
                @Html.TextBoxFor(t => t.SearchString, new { @class = "form-control" })
            </td>
            <td>
                @Html.Label("AdvancedSearch", "Advanced?", new { @class = "form-control" })
            </td>
            <td>
                @Html.CheckBoxFor(t => t.AdvancedSearch, new { @class = "form-control" })
            </td>
        </tr>
    </table>
    ```

26. In Solution Explorer, click **Web.config**.
27. On the **Web.config** tab, specify values for the following settings:
    - **EndpointUrl**. Replace **\<COSMOS DB URL\>** with the **URI** you noted earlier from the **20777a-mod6-sql-<*your initials and day*>** Cosmos DB account.
    - **PrimaryKey**. Replace **\<COSMOS DB KEY\>** with the **PRIMARY KEY** you noted earlier from the**20777a-mod6-sql-<*your initials and day*>** Cosmos DB account.
    - **SearchService**. Replace **\<SEARCH SERVICE\>** with **20777a-search-\<*your name\>-\<the day*\>**.
    - **SearchServiceQueryKey**. Replace **\<SEARCH SERVICE QUERY KEY\>** with the **\<empty\>** query key for your Azure Search Service.
28. On the **Build** menu, click **Build Soultion**.
29. Press F5 to run the app.
30. In Internet Explorer, in the **Search for Products** section, in the text box, type **natural**, and then click **Search**. Verify that the search returns two products (**Mountain Bike Socks, M**, and **Mountain Bike Socks, L**).
31. On either row, click **Details**. The **Description** contains the following text:

    ```Text
    Combination of natural and synthetic fibers stays dry and provides just the right cushioning.
    ```

32. Click **Adventure-Works Product Catalog**.
33. In the **Search for Products** section, in the text box, type **Bib-Shorts**, and then click **Search**. Verify the results should list three products (**Men's Bib-Shorts, S**, **Men's Bib-Shorts, M**, and **Men's Bib-Shorts, L**).
34. Select the **Advanced?** check box, and then click **Search**. This time the search returns more products. The search string has been interpreeted as a query expression, and now finds all products that contain the text **Bib** and/or **Shorts**
35. Experiment with other searches. When you have finished, close the app and return to Visual Studio.

### Task 4: Use the Suggester in the Search Field

1. In Visual Studio, on the **Repository.cs** tab, find the **GetSuggestions** method. This method takes a string parameter, and will use the Azure Search Service Suggester that you created earlier to suggest matching terms from the search index.
2. After the **// TODO: Specify the Suggestion parameters: enable fuzzy searching, fetch the top 10 results, and add highlighting** comment, add the following code:

    ```CSharp
    var parameters = new SuggestParameters
    {
        UseFuzzyMatching = true,
        Top = 10,
        HighlightPreTag = "<b>",
        HighlightPostTag = "</b>"
    };
    ```

3. After the **// TODO: Retrieve suggestions from the Azure Search service** comment, add the following statement:

    ```CSharp
    DocumentSuggestResult result = await searchClient.Documents.SuggestAsync(term, suggesterName, parameters);
    ```

4. After the **// TODO: Return the suggestions as a list of strings** comment, replace the code that throws the **NotImplementedException** with the following statement. This statement extracts the **Text** field from each of the results and converts these items into a list.

    ```CSharp
    return result.Results.Select(x => x.Text).ToList();
    ```

5. On the **ProductsController.cs** tab, find the **SuggestAsync** method shown below. This method responds to the **Suggest** HTTP GET request. The parameter is a string on which to base suggestions.

    ```CSharp
    // TODO: Method called by the autocomplete feature of the form to use the Suggester in Azure Search to "suggest" search strings
    [ActionName("Suggest")]
    [HttpGet]
    public async Task<ActionResult> SuggestAsync(string term)
    {
        // TODO: Call the GetSuggestions method in repository to find suggestions that match the term provided

        // TODO: Return the list of suggestions
        throw new NotImplementedException();
    }
    ```

6. After the **// TODO: Call the GetSuggestions method in repository to find suggestions that match the term provided** comment, add the following statement:

    ```CSharp
    var suggestions = await Repository<Product>.GetSuggestions(term);
    ```

7. After the **// TODO: Return the list of suggestions** comment, replace the code that throws the **NotImplementedException** with the following statement:

    ```CSharp
    return new JsonResult
    {
        JsonRequestBehavior = JsonRequestBehavior.AllowGet,
        Data = suggestions
    };
    ```

8. On the **FindProducts.cshtml** tab, near the top of the file, find the **\<!-- TODO: Add jquery-ui stylesheet and function to add autocomplete to the SearchString text field--\>** comment. You will use the **autocomplete** feature of the jquery-ui package to send the **Suggest** request to the controller as te user enters data into the **Search** field.
9. After the **\<!-- TODO: Add jquery-ui stylesheet and function to add autocomplete to the SearchString text field--\>** comment, add the following markup to the view. This code adds a reference to the style-sheet that is used by the jquery-ui package.

    ```HTML
    <link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
    ```

10. Inside the **\<script\>** tag, after the **// TODO: Replace anything that looks like HTML markup from the SearchString field after performing autocomplete** comment, add the following JavaScript function:

    ```JavaScript
    var updateTextbox = function (event, ui) {
        var result = ui.item.value.replace(/<\/?[^>]+(>|$)/g, "");
        $("#SearchString").val(result);
        return false;
    };
    ```

11. After the **// TODO: Trigger autocomplete on the SearchString field to invoke the Suggest method on the form. The data typed so far is provided as an input parameter to the method** comment, add the following code. The call to the jquery-ui **autocomplete** function invokes the **Suggest** action once the user has entered three or more characters. The JSON data returned by the **Suggest** is displayed as a drop-down list box by the **autocomplete** function. When the user selects an item from the list, any HTML tags (such as those generated by Azure Suggester highlighting) are stripped out using the **updateTextbox** function as the item is written to the **SearchString** field.

    ```JsvaScript
    $("#SearchString").autocomplete({
        html: true,
        source: "/Products/Suggest",
        minLength: 3,
        select: updateTextbox,
        focus: updateTextbox
    }).data("ui-autocomplete")._renderItem = function (ul, item) {
        return $("<li></li>")
            .data("item.autocomplete", item)
            .append("<a>" + item.label + "</a>")
            .appendTo(ul);
        };
    ```

11. On the **Build** menu, click **Build Soultion**.
13. Press F5 to run the app.
14. In Interent Explorer, under **Search for Products**, in the text box, type **nat**. A list of suggestions should appear, with the matching text highlighted in bold. Note that similar items are included that are not necessarily an exact match for the text; this is because the repository uses fuzzy matching with the suggester.
15. Change the text to **natu**, and the list should be refined to two entries.
16. Click one of the entries, and then click **Search**. The result should be two products.
17. Next to one of the products, click **Details**, and you will see that the products description contains the word **natural**.
18. Click **Adventure-Works Product Catalog**.
19. In the **Search for Products** section, in the text box, type **bib**.
20. Select **Men's Bib-Shorts, M**, and then click **Search**. This time only a single product should be displayed.

    > **Note**: If you select **Advanced** and click **Search**, you will get more results. This is because the text in the search box is now interpreted as a query expression, and some of the characters have special meanings.
21. Experiment with other suggestions and searches. Close the app when you have finished.

## Exercise 2: Generating Month-end Order Summaries

### Task 1: Create an HDInsight Cluster for Running Spark

1. In Internet Explorer, in the Azure Portal, in the left panel, click **+ Create a resource**.
2. On the **New** blade, click **Analytics**, and then click **HDInsight**.
3. On the **HDInsight** blade, click **Custom (size, settings, apps)**.
4. On the **Basics** blade, enter the following details, and then click **Cluster type**:
   - **Cluster name**: hdi-\<*your name\>\<date*\>
   - **Subscription**: your subscription
5. On the **Cluster configuration** blade, enter the following details, and then click **Select**:
   - **Cluster type**: Spark
   - **Version**: Spark 2.3.0 (HDI 3.6)
6. On the **Basics** blade, enter the following details, and then click **Next**:
   - **Cluster login username**: sparkadmin
   - **Cluster login password**: Pa55w.rdPa55w.rd
   - **Secure Shell (SSH) username**: sadmin
   - **Use same password as cluster login**: selected
   - **Resource group (Use existing)**: 20777_Mod06
   - **Location**: Select your region
7. On the **Security + networking** blade, click **Next**.
8. On the **Storage** blade, under **Select a Storage account**, click **Create new**.
9. In the **Create a new Storage account** box, type **\<*your name\>\<date*\>sa**.
10. In the **Default container** box, replace the suggested name with the name of your cluster.
11. Leave the other settings at their defaults, and then click **Next**.
12. On the **Applications (optional)** blade, click **Next**.
13. On the **Cluster size** blade, in the **Number of Worker nodes** box, type **2**.
14. Click **Worker node size**.
15. On the **Choose your node size** blade, click **D12 V2 Optimized**, and then click **Select**.
16. Click **Head node size**.
17. On the **Choose your node size** blade, click **D12 V2 Optimized**, and then click **Select**.
18. On the **Cluster size** blade, click **Next**.
19. On the **Script actions** blade, click **Next**.
20. On the **Cluster summar**y blade, click **Create**.
21. In the left panel, click **All resources**, and then click **hdi-\<*your name\>\<date*\>**.
22. Wait for the cluster to be provisioned and the status to show as **Running**. This is likely to take at least 10 minutes. You can continue with the next task while cluster is being built.

### Task 2: Upload Sample Orders to the Cosmos DB Database

1. In Visual Studio, on the **File** menu, point to **Open**, and then click **Project/Solution**.
2. In the **Open Project** dialog box, naviagate to the **E:\\Labfiles\\Lab06\\Data\\GenerateOrderData** folder, and then double-click **GenerateOrderData.sln**.
3. If the **Security Warning for MigrateProductData** dialog box appears, clear the **Ask me for every project in this solution** check box, and then click **OK**.
4. In Solution Explorer, click **App.config**.
5. On the **App.config** tab, in the **appSettings** section, specify the following configuration settings:
    - **EndpointUrl**. Replace **\<URI\>** with the **URI** you noted earlier from the **20777a-mod6-sql-<*your initials and day*>** Cosmos DB account.
    - **PrimaryKey**. Replace **\<KEY\>** with the **PRIMARY KEY** you noted earlier from the **20777a-mod6-sql-<*your initials and day*>** Cosmos DB account.
6. On the **Build** menu, click **Build Solution**.
7. Press F5 to run the application. The orders will be generated quickly, but might take a minute or two to be uploaded to the database.
8. In Internet Explorer, in the left panel, click **All resources**, and then click **20777a-mod6-sql-<*your initials and day*>**.
9. On the **20777a-mod6-sql-<*your initials and day*>** blade, click **Data Explorer**, and then expand **Adventure-Works**.
10. Right-click **Data**, and then click **New SQL Query**.
11. On the **Query 1** tab, type the following query:

    ```SQL
    SELECT * FROM c WHERE c.doctype = "ShoppingCartOrder"
    ```

12. Click **Execute Query**, and examine the first few orders that have been created.
13. Change the query as follows:

    ```SQL
    SELECT VALUE COUNT(1) FROM c WHERE c.doctype = "ShoppingCartOrder"
    ```

14. Click **Execute Query**, and verify that the database contains at least **5000** orders (possibly a few more).

### Task 3: Install and Configure PuTTY

1. In File Explorer, go to **E:\\**, and then create a new folder called **putty**.
2. In Internet Explorer, browse to **https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html**.
3. In the **Alternative binary files** section, under **putty.exe**, next to **64-bit**, click **putty.exe**.
4. In the message box, click the **Save** drop-down arrow, and then click **Save as**.
5. In the **Save As** dialog box, in the **File name** box, type **E:\\putty\\putty.exe**, and then click **Save**.
6. In the **Alternative binary files** section, under **pscp.exe**, next to **64-bit**, click **pscp.exe**.
7. In the message box, click the **Save** drop-down arrow, and then click **Save as**.
8. In the **Save As** dialog box, in the **File name** box, type **E:\\putty\\pscp.exe**, and then click **Save**.
9. Close the PuTTY tab.
10. In Internet Explorer, in the left panel, click **All resources**, and then click **hdi-\<*your name\>\<date*\>**.
11. On the **hdi-\<*your name\>\<date*\>** blade, verify that the HDInsight cluster has been created. If not, wait for provisioning to complete.
12. On the **hdi-\<*your name\>\<date*\>** blade, under **Settings**, click **SSH + Cluster login**.
13. On the **SSH + Cluster login** blade, in the **Hostname** drop-down list, click **hdi-\<*your name\>\<date*\>-ssh.azurehdinsight.net**, and then click the **Click to copy** button (two document icons on a blue background).
14. In File Explorer, go to **E:\putty**, and then double-click **putty.exe**.
15. In the **Open File - Security Warning** dialog box, click **Run**.
16. In the **PuTTY Configuration** window, right-click the **Host Name (or IP Address)** box, and then click **Paste**.
17. Edit the pasted value to remove **ssh sadmin@** at the beginning of the host name.
18. In the **Saved Sessions** box, type **HDInsight**, click **Save**, and then click **Open**.
19. In the **PuTTY Security Alert** dialog box, click **Yes**.
20. In the PuTTY window, at the **login as** prompt, type **sadmin**, and then press Enter.
21. At the **password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.
22. If login is successful, you will be presented with a prompt that starts **sadmin@hn0**.

### Task 4: Perform Interactive Queries Against Cosmos DB Using Spark

1. At the PuTTY prompt, type the following command, press Enter, and wait for the Spark shell to start running:

    ```Bash
    spark-shell --master yarn --packages com.microsoft.azure:azure-cosmosdb-spark_2.3.0_2.11:1.2.0
    ```

2. At the **scala>** prompt, type the following commands:

    ```Scala
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark._
    import com.microsoft.azure.cosmosdb.spark.config._
    ```

3. At the **scala>** prompt, type the following command that creats a configuration object that you will use to retrieve the orders held in the Cosmos DB database. Replace **\~URL\~** with the **URL**, and replace **\~KEY\~** with the **PRIMARY KEY** for the Cosmos DB account.

    ```Scala
    val databaseConfig = Config(Map(
        "Endpoint" -> "~URL~",
        "Masterkey" -> "~KEY~",
        "Database" -> "Adventure-Works",
        "Collection" -> "Data",
        "query_custom" -> "SELECT * FROM c WHERE c.doctype = 'ShoppingCartOrder'"
    ))
    ```

4. At the **scala>** prompt, type the following command. This command executes the query and creats a temporary Hive view called **orders** that references the results (the data in the view is not persisted).

    >**Note**: You will receive a warning specifying that JNI has not been loaded. You can ignore this warning.

    ```Scala
    val collection = spark.sqlContext.read.cosmosDB(databaseConfig)
    collection.createOrReplaceTempView("orders")
    ```

5. At the **scala>** prompt, type the following query, and verify that it finds at least 5000 rows. Note that the query runs as a Spark job.

    ```Scala
    val data = spark.sql("SELECT * FROM orders")
    data.count()
    ```

6. At the **scala>** prompt, type the following command. This command shows the schema generated for the orders view, based on the data returned by the query in the configuration object.

    ```Scala
    data.printSchema
    ```

7. Notice that the **orderitems** field is structured as an array in the view:

    ```Text
    root
    |-- _attachments: string (nullable = true)
    |-- _etag: string (nullable = true)
    |-- _rid: string (nullable = true)
    |-- _self: string (nullable = true)
    |-- _ts: integer (nullable = true)
    |-- customerdiscountrate: integer (nullable = true)
    |-- dateplaced: long (nullable = true)
    |-- doctype: string (nullable = true)
    |-- id: string (nullable = true)
    |-- isshoppingcartororder: string (nullable = true)
    |-- itemscost: double (nullable = true)
    |-- lastupdated: long (nullable = true)
    |-- numberofitems: integer (nullable = true)
    |-- orderitems: array (nullable = true)
    |    |-- element: struct (containsNull = false)
    |    |    |-- backorderreference: string (nullable = true)
    |    |    |-- lineitemtotalcost: double (nullable = true)
    |    |    |-- numberincartorordered: integer (nullable = true)
    |    |    |-- numberonbackorder: integer (nullable = true)
    |    |    |-- productid: string (nullable = true)
    |    |    |-- productname: string (nullable = true)
    |    |    |-- productnumber: string (nullable = true)
    |    |    |-- subcategory: string (nullable = true)
    |    |    |-- unitcost: double (nullable = true)
    |-- orderstatus: string (nullable = true)
    |-- partitionkey: string (nullable = true)
    ...
    |-- totalcost: double (nullable = true)
    |-- ttl: integer (nullable = true)

    ```

8. At the **scala>** prompt, type the following query. This query returns the name of the first product in each order together with the number ordered (the output is truncated to display only the first 20 orders).

    ```Scala
    spark.sql("SELECT orderitems[0].productname, orderitems[0].numberincartorordered FROM orders").show()
    ```

9. At the **scala>** prompt, type the following query. This query reports the total value of orders that are in progress, that have been delivered, and that have been cancelled.

    ```Scala
    spark.sql("SELECT SUM(totalcost), orderstatus FROM orders GROUP BY orderstatus").show()
    ```

10. Experiment by running other queries against the **orders** view. When you have finished, type **:quit**, and then press Enter to close the spark shell.

    >**Note**: If the spark shell does not close automatically, close the window manually using the **X** in the top right corner, and then click **OK**.

### Task 5: Perform Batch Operations by Using a Jupyter Notebook

1. On the Start menu, type **cmd**, and then press Enter.
2. At the command prompt, type **E:**, and then press Enter.
3. At the command prompt, type **cd "E:\\Labfiles\\Lab06\\Starter\\Exercise 2"**, and then press Enter.
4. At the command prompt, type the following command, and then press Enter. This command uses the pscp utility to upload the jar file containing the Cosmos DB connector for Spark to the first head node of your HDInsight cluster. Replace **\<cluster-name\>** with **hdi-\<*your name\>\<date*\>**.

    ```Script
    E:\putty\pscp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar sadmin@<cluster-name>-ssh.azurehdinsight.net:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    ```

5. At the **Password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.
6. In File Explorer, go to **E:\putty**, and then double-click **putty.exe**.
7. In the **Open File - Security Warning** dialog box, click **Run**.
8. In the **PuTTY Configuration** window, under **Saved Sessions**, click **HDInsight**, click **Load**, and then click **Open**.
9. In the PuTTY window, at the **login as** prompt, type **sadmin**, and then press Enter.
10. At the **password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.
11. If login is successful, you will be presented with a prompt that starts **sadmin@hn0**.
12. At the putty prompt, type the following command, and then press Enter:

    ```Bash
    ls
    ```

13. Verify that the file **azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar** is listed.
14. At the putty prompt, type the following command, and then press Enter to copy the Cosmos DB connector to the folder required by Spark applications:

    ```Bash
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    ```

15. At the putty prompt, type the following commands to copy the Cosmos DB connector to the same folder on the remaining head node of the HDInsight cluster. At the **Are you sure you want to continue connecting** prompt, type **yes**, and then press Enter. At the **password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.

    ```Bash
    hn=`hostname` && hn1="${hn/hn0-/hn1-}"
    rcp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar $hn1:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    ssh $hn1
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    exit
    ```

16. At the putty prompt, type the following commands to copy the Cosmos DB connector to the same folder on the first worker node of the HDInsight cluster. At the **Are you sure you want to continue connecting** prompt, type **yes**, and then press Enter. At the **password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.

    ```Bash
    hn=`hostname` && wn0="${hn/hn0-/wn0-}"
    rcp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar $wn0:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    ssh $wn0
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    exit
    ```

17. At the putty prompt, type the following commands to copy the Cosmos DB connector to the same folder on the second worker node of the HDInsight cluster. At the **Are you sure you want to continue connecting** prompt, type **yes**, and then press Enter. At the **password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.

    > **Note**: This step may cause the error **Name or service not known**. If this occurs, ignore this step and perform step 18 instead. If this step succeeds, then skip step 18.

    ```Bash
    hn=`hostname` && wn1="${hn/hn0-/wn1-}"
    rcp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar $wn1:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    ssh $wn1
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    exit
    ```

18. At the putty prompt, type the following commands to copy the Cosmos DB connector to the same folder on the second worker node of the HDInsight cluster. At the **Are you sure you want to continue connecting** prompt, type **yes**, and then press Enter. At the **password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.

    ```Bash
    hn=`hostname` && wn2="${hn/hn0-/wn2-}"
    rcp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar $wn2:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    ssh $wn2
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    exit
    ```

19. In Internet Explorer, in the left panel, click **All resources**, and then click **hdi-\<*your name\>\<date*\>**.
20. On the **hdi-\<*your name\>\<date*\>** blade, in the **Cluster dashboards** section, click **Ambari home**.
21. In the **Windows Security** dialog box, in the **User name** box, type **sparkadmin**.
22. In the **Password** box, type **Pa55w.rdPa55w.rd**, and then click **OK**.
23. On the **Ambari** page, in the menu at the top, click **Services**, and then click **Spark2**.
24. In the **Service Actions** drop-down list, click **Restart All**.
25. In the **Confirmation** dialog box, click **Confirm Restart All**.
26. In the **Background Operation Running** dialog box, wait for the Spark2 components to restart, and then click **OK**.
27. Close the Ambari page.
28. In the Azure portal, on the **hdi-\<*your name\>\<date*\>** blade, in the **Cluster dashboards** section, click **Jupyter notebook**.
29. On the **Jupyter** page, on the **New** drop-down list, click **Spark**. This action creates a Scala notebook that can perform Spark queries.

    >**Note**: If the message error **404 : Not Found** appears, wait a few seconds, and then refresh the page.

30. In the page heading, click **Untitled**.
31. In the **Rename Notebook** dialog box, type **Monthly Analysis**, and then click **OK**.

    >**Note**: The complete code for the notebook is available in the file **E:\\Labfiles\\Lab06\\Solution\\Exercise 2\\Monthly+Analysis.txt**.

32. In the first cell of the notebook, add the following Scala code to create a useful utility class that can convert between datetime values and ticks (as used by the documents in the CosmosDB database):

    ```Scala
    // Helper class for converting between dates and ticks

    import java.util.Calendar
    import java.util.Date

    object DateHelper {
        private final val TICKS_AT_EPOCH = 621355968000000000L
        private final val TICKS_PER_MILLISECOND = 10000

        def getUTCTicks(date: Date): Long = {
            val calendar = Calendar.getInstance
            calendar.setTime(date)

            return (calendar.getTimeInMillis() * TICKS_PER_MILLISECOND) + TICKS_AT_EPOCH
        }

        def getDate(ticks: Long): Date = {
            return new Date((ticks - TICKS_AT_EPOCH) / TICKS_PER_MILLISECOND)
        }
    }
    ```

33. On the **Insert** menu, click **Insert Cell Below**.
34. In the new cell, add the following code:

    ```Scala
    // Get the time in ticks one month ago

    val calendar = Calendar.getInstance
    calendar.add(Calendar.MONTH, -1)
    val then = calendar.getTime
    val thenInTicks = DateHelper.getUTCTicks(then)
    ```

35. On the **Insert** menu, click **Insert Cell Below**.
36. In the new cell, add the following code to import the libraries required by the Cosmos DB connector:

    ```Scala
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark._
    import com.microsoft.azure.cosmosdb.spark.config._
    ```

37. On the **Insert** menu, click **Insert Cell Below**.
38. In the new cell, add the following code. This uses an %%sql magic to drop the orders table in the Hive database if it currently exists. This step is nessary as you will create a virtual table, named orders, that references the orders in the Data collection in the CosmosDB database.

    ```SQL
    %%sql
    -- Drop the orders table if it currently exists
    DROP TABLE IF EXISTS orders
    ```

39. On the **Insert** menu, click **Insert Cell Below**.
40. Add the following SQL magic to the new cell. Replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY ACCESS** key of your **20777a-mod6-sql-\<*your initials and day*\>** Cosmos DB acount. This code creates a virtual table that references the orders data in the Cosmos DB database directly rather than by creating a Hive view (the data is not copied, rather the query is performed everytime the table is accessed).

    ```SQL
    %%sql
    -- Rebuild the orders table as a reference to the Data collection in the Cosmos DB database
    -- Don't include the orderitems subdocuments - these are arrays and are better handled as a seperate external table

    CREATE TABLE orders using com.microsoft.azure.cosmosdb.spark options (
    endpoint "~URI~",
    masterkey "~KEY~",
    database "Adventure-Works",
    collection "Data",
    query_custom "SELECT c.id, c.numberofitems, c.itemscost, c.customerdiscountrate, c.totalcost, c.dateplaced, c.orderstatus, c.lastupdated FROM c WHERE c.doctype = 'ShoppingCartOrder'")
    ```

41. On the **Insert** menu, click **Insert Cell Below**.
42. In the new cell, add the following code. This drops the orderitems table in the Hive database. You will create another table that contains the order items from all of the orders.

    ```SQL
    %%sql
    -- Drop the orderitems table if it currently exists
    DROP TABLE IF EXISTS orderitems
    ```

43. On the **Insert** menu, click **Insert Cell Below**.
44. Add the following SQL magic to the new cell. Replace **\~URI\\~** with the **URI**, and replace **\~KEY\\~** with the **PRIMARY ACCESS** key of your **20777a-mod6-sql-\<*your initials and day*\>** Cosmos DB acount. This code creates another virtual table that references the order items for each order in the Cosmos DB database. This data is held as subdocuments, and each subdocument contains an array of order items. The query that fetches the data for this table uses a CosmosDB JOIN clause to flatten this data out into a simple table which makes the data easier to handle using Spark SQL.

    ```SQL
    %%sql
    -- Get the orderitems for orders placed in the last month. The JOIN clause flattens the arrays in the subdocuments into a tabular format

    CREATE TABLE orderitems using com.microsoft.azure.cosmosdb.spark options (
    endpoint "~URI~",
    masterkey "~KEY~",
    database "Adventure-Works",
    collection "Data",
    query_custom "SELECT o.id, o.dateplaced, o.orderstatus, i.productnumber, i.productname, i.numberincartorordered, i.lineitemtotalcost FROM o JOIN i IN o.orderitems WHERE o.doctype = 'ShoppingCartOrder'")
    ```

45. On the **Insert** menu, click **Insert Cell Below**.
46. Add the following Scala code to the new cell to create a Spark dataframe that retrieves all documents from the CosmosDB collection (via the orders virtual table) for orders that have been placed in the last month:

    ```Scala
    // Find the status and cost of all orders placed in the last month
    // Note: Only retrieve the necessary columns (not "**"), to keep the dataframe lean

    val orders = spark.sqlContext.sql(s"SELECT orderstatus, totalcost FROM orders WHERE dateplaced > $thenInTicks")
    ```

47. On the **Insert** menu, click **Insert Cell Below**.
48. Add the following  Scala code to the new cell to create a new dataframe that summarizes the data just fetched, to generate a count of the number of orders in the last month, grouped by order status:

    ```Scala
    // Find the total number of orders, grouped by order status ("In progress", "Delivered", "Cancelled")

    val numbersByStatus = orders.groupBy("orderstatus").count()
    numbersByStatus.show
    ```

49. On the **Insert** menu, click **Insert Cell Below**.
50. Add the following Scala code to the new cell. This code generates a dataframe that shows the total revenue grouped by order status. Using this dataframe you can quickly see how much revenue has been generated by delivered orders and orders currently in progress, as well as determine how much money was refunded due to cancelled orders.

    ```Scala
    // Find the sum value of the revenue for all orders, grouped by order status

    val revenuesByStatus = orders.groupBy("orderstatus").sum("totalcost")
    revenuesByStatus.show
    ```

51. On the **Insert** menu, click **Insert Cell Below**.
52. Add the following Scala code to the new cell. This code generates a dataframe that summarizes information about the items ordered in the last month, retrieved using the orderitems virtual table. You can use this dataframe to establish which were the most popular products, and which products generated the most revenue.

    ```Scala
    // Find the items ordered in the last month

    val orderitems = spark.sqlContext.sql(s"SELECT productnumber, productname, SUM(numberincartorordered) AS numordered, SUM(lineitemtotalcost) AS revenue FROM orderitems WHERE dateplaced > $thenInTicks AND orderstatus <> 'Cancelled' GROUP BY productnumber, productname")
    orderitems.show(5)
    ```

53. On the **Insert** menu, click **Insert Cell Below**.
54. Add the following Scala code to the new cell. This code sorts the order item summary records by number ordered. The result shows the products in decreasing order or popularity.

    ```Scala
    // Sort the data in descending order of volume (numordered)

    val orderedByVolume = orderitems.sort($"numordered".desc)
    orderedByVolume.show(5)
    ```

55. On the **Insert** menu, click **Insert Cell Below**.
56. Add the following Scala code to the new cell to sort the order item summary records by revenue:

    ```Scala
    // Sort the data in descending order of revenue per product

    val orderedByRevenue = orderitems.sort($"revenue".desc)
    orderedByRevenue.show(5)
    ```

57. On the **Insert** menu, click **Insert Cell Below**.
58. Add the following Scala code to the new cell. This code configures a connection that you will use to write the summary data just generated back to the Cosmos DB database. Replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY ACCESS** key of your **20777a-mod6-sql-\<*your initials and day*\>** Cosmos DB acount.

    ```Scala
    // Write the summaries back to Cosmos DB with doctype "MonthlySummary"

    import org.apache.spark.sql._

    val writeConfigMap = Map(
        "Endpoint" -> "~URI~",
        "Masterkey" -> "~KEY~",
        "Database" -> "Adventure-Works",
        "Collection" -> "Data"
    )

    val writeConfig = Config(writeConfigMap)
    ```

59. On the **Insert** menu, click **Insert Cell Below**.
60. Add the following Scala code to the new cell. This code creates a text value containing the month and year. You will add this value to the summary data.

    ```Scala
    val month = calendar.get(Calendar.MONTH) + 1
    val year = calendar.get(Calendar.YEAR)
    val monthYear = s"$month/$year"
    ```

61. On the **Insert** menu, click **Insert Cell Below**.
62. Add the following Scala code to the new cell. This code augments the **"orders by status"** dataframe with three columns; **doctype ("MonthlySummary")**, **month (the text value just created)**, and **partitionKey ("summary")**. It then uses the connection to the Cosmos DB database to write this data as a document back to the Cosmos DB database.

    ```Scala
    val numbers = numbersByStatus.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    numbers.write.mode(SaveMode.Append).cosmosDB(writeConfig)
    numbers.show(5)
    ```

63. On the **Insert** menu, click **Insert Cell Below**.
64. Add the following Scala code to the new cell. This code is similar to the previous cell except that it saves the data in the "revenues by status" dataframe.

    ```Scala
    val revenues = revenuesByStatus.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    revenues.write.mode(SaveMode.Append).cosmosDB(writeConfig)
    revenues.show(5)
    ```

65. On the **Insert** menu, click **Insert Cell Below**.
66. Add the following Scala code to the new cell to save the dataframe that shows the popularity of products:

    ```Scala
    val itemsVolume = orderedByVolume.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    itemsVolume.write.mode(SaveMode.Append).cosmosDB(writeConfig)
    itemsVolume.show(5)
    ```

67. On the **Insert** menu, click **Insert Cell Below**.
68. Add the following Scala code to the new cell. This code displays a dataframe that shows the revenue generated by product. You do not need to save this data, as it is the same as that generated by the previous dataframe, just in a different order.

    ```Scala
    val itemsRevenue = orderedByRevenue.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    itemsRevenue.show(5)
    ```

69. On the **File** menu, click **Save and Checkpoint**.
70. On the **Cell** menu, click **Run All**. You should see the results output by each cell as it is executed.

    > **Note**: With the small-scale Spark configuation used by this lab (to save costs), the final few cells that write the data back to Cosmos DB can take some time to run. You should continue with the next exercise while the notebook executes, and come back to verify the results later.

71. In the Azure portal, in the left panel, click **All resources**, and then click **20777a-mod6-sql-\<*your initials and day*\>**.
72. On the **20777a-mod6-sql-\<*your initials and day*\>** blade, click **Data Explorer**.
73. In the **SQL API** pane, expand **Adventure-Works**, right-click **Data** , and then click **New SQL Query**.
74. Enter the following query, and then click **Execute Query**:

    ```SQL
    SELECT * FROM c WHERE c.doctype = "MonthlySummary"
    ```

75. Verify that a set of summary documents appear. The example below shows the typical documents showing the number of orders by order status:

    ```JSON
    [
        {
            "doctype": "MonthlySummary",
            "orderstatus": "Delivered",
            "month": "7/2018",
            "partitionKey": "summary",
            "count": 236,
            "id": "84c0b06a-4191-4859-b5d6-9cd434bd5a71",
            "_rid": "CyxpAK2n81T2HQAAAAAAAA==",
            "_self": "dbs/CyxpAA==/colls/CyxpAK2n81Q=/docs/CyxpAK2n81T2HQAAAAAAAA==/",
            "_etag": "\"0000ef1f-0000-0000-0000-5b6af6f00000\"",
            "_attachments": "attachments/",
            "_ts": 1533736688
        },
        {
            "doctype": "MonthlySummary",
            "orderstatus": "In progress",
            "month": "7/2018",
            "partitionKey": "summary",
            "count": 309,
            "id": "02727670-622b-420d-9790-36354c0b77a7",
            "_rid": "CyxpAK2n81T3HQAAAAAAAA==",
            "_self": "dbs/CyxpAA==/colls/CyxpAK2n81Q=/docs/CyxpAK2n81T3HQAAAAAAAA==/",
            "_etag": "\"0000f01f-0000-0000-0000-5b6af6f10000\"",
            "_attachments": "attachments/",
            "_ts": 1533736689
        },
        {
            "doctype": "MonthlySummary",
            "orderstatus": "Cancelled",
            "month": "7/2018",
            "partitionKey": "summary",
            "count": 46,
            "id": "e1f8ce67-749b-4cbd-abef-ec165b8fff3c",
            "_rid": "CyxpAK2n81T4HQAAAAAAAA==",
            "_self": "dbs/CyxpAA==/colls/CyxpAK2n81Q=/docs/CyxpAK2n81T4HQAAAAAAAA==/",
            "_etag": "\"0000f11f-0000-0000-0000-5b6af6f50000\"",
            "_attachments": "attachments/",
            "_ts": 1533736693
        },
        ...
    ]
    ```

## Exercise 3: Visualizing Sales Data

### Task 1: Create an Azure Databricks Service and Cluster

1. In the Azure Portal, click **Create a resource**, in the search box, type **Azure Databricks**, and then press Enter.
2. On the **Everything** blade, click **Azure Databricks**, and then click **Create**.
3. On the **Azure Databricks Service** blade, in the **Workspace name** box, type **20777a-databricks-\<*your name\>-\<the day*\>**, for example, **20777a-databricks-john-31**.
4. Under **Resource group**, click **Use existing**, and then in the drop-down list, click **20777_Mod06**.

    > **Note**: The Azure Databricks service creates a significant number of resources. It places them in a seperate resource group with a name based on the name that you select here.

5. In the **Location** drop-down list, click **West US 2**.

    > **Note**: Currently, not all regions support the range of VMs used by Azure Databricks to host Spark clusters. West US 2 does.

6. In the **Pricing Tier** drop-down list, click **Trial (Premium - 14-Days Free DBUs)**, and then click **Create**.
7. Wait until the Azure Databricks resource is created.
8. In the left panel, click **All resources**, and then click **20777a-databricks-\<*your name\>-\<the day*\>**.
9. On the **20777a-databricks-\<*your name\>-\<the day*\>** blade, click **Launch Workspace**.
10. On the **Azure Databricks** blade, under **Common Tasks**, click **New Cluster**.
11. On the **New Cluster** page, in the  **Cluster Name** box, type **\<your name\>-cluster**.
12. In the **Max Workers** box, type **4**, leave all other settings at their default values, and then click **Create Cluster**.
13. Wait while the cluster is created and started. Verify that its **State** is set to **Running** before continuing.

### Task 2: Create a Library and PyDocumentDB Notebook

1. In the left panel, click **Azure Databricks**.
2. On the **Azure Databricks** blade, under **Common Tasks**, click **Import Library**.
3. On the **Create Library** page, under **Library Source**, click **PyPI**.
4. In the **Package** box, type **pydocumentdb**, and then click **Create**.
5. On the **pydocumentdb** page, in the **\<*your name*\>-cluster** row, select the check box, click **Install**, and wait until the status changes to **Installed**.
6. In the left panel, click **Azure Databricks**.
7. On the **Azure Databricks** blade, under **Common Tasks**, click **Import Library**.
8. On the **Create Library** blade, under **Library Type**, click **Jar**.
9. In the **Library Name** box, type **Cosmos DB Connector**.
10. In the **JAR File** box, click **Drop JAR here**.
11. In the **Choose File to Upload** dialog box, go to **E:\Labfiles\Lab06\Starter\Exercise 2**, click **azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar**, and then click **Open**.
12. Wait for the JAR file to be uploaded, and then click **Create**.
13. On the **Cosmos DB Connector** page, select the **\<your name\>-cluster** check box, click **Install**, and then wait while the library is attached to the cluster; the **Status** should change to **Installed**.
14. In the left panel, click **Azure Databricks**.
15. On the **Azure Databricks** blade, under **Common Tasks**, click **New Notebook**.
16. In the **Create Notebook** dialog box, in the **Name** box, type **Analyze Orders**.
17. In the **Language** box, click **Python**, and then click **Create**.

    > **Note**: This exercise uses Python and PyDocumentDB, which is suitable for processing moderate amounts of data. If you have a large and growing volume of data, you should consider creating a Scala notebook with the Cosmos DB Connector, which can provide a more scalable solution.

### Task 3: Write Python Code to Retrieve and Display the Data

1. In the first cell of the notebook, enter the following code that specifies the modules that will be used by the notebook:

    ```Python
    # Import modules

    import pydocumentdb
    from pydocumentdb import document_client
    from pydocumentdb import documents
    import datetime
    from datetime import datetime
    ```

2. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**. Verify that the code in the cell runs without any errors.
3. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
4. In the new empty cell, enter the following code:

    ```Python
    # Work out what this time last month was, in ticks

    thisTimelastMonthInTicks = (datetime.utcnow() - datetime(1,1,1)).total_seconds() * 1000000
    ```

5. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.
6. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
7. In the new empty cell, enter the following code that defines the query that the notebook will use to retrieve the data from Cosmos DB:

    ```Python
    # Define the query to retrieve the orders data from Cosmos DB

    query = 'SELECT o.id, o.dateplaced, o.orderstatus, i.productnumber, i.productname, i.numberincartorordered, i.lineitemtotalcost FROM o JOIN i IN o.orderitems WHERE o.doctype = \\\'ShoppingCartOrder\\\' AND o.dateplaced > ' + str(thisTimelastMonthInTicks)
    ```

8. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.
9. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
10. In the new empty cell, enter the following code that specifies the command that Spark will use to create a virtual table that it can use to fetch the data from Cosmos DB. Replace **--URI--** with the **URI**, and **--PRIMARY KEY--** with the **PRIMARY KEY** you noted earlier from the **20777a-mod6-sql-<*your initials and day*>** Cosmos DB account.

    ```Python
    # Specify the Spark SQL command to create a virtual table that references the data defined by the query in the Cosmos DB collection

    command = "create table orderdata using com.microsoft.azure.cosmosdb.spark options (endpoint '--URI--', masterkey '--KEY--', database 'Adventure-Works', collection 'Data', query_custom '" + str(query) + "')"
    ```

11. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.
12. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
13. In the new empty cell, enter the following code that removes the **orderdata** table from the database if it already exists:

    ```Python
    # Remove any existing version of the orderdata table

    spark.sql('drop table if exists orderdata')
    ```

14. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.
15. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
16. In the new empty cell, enter the following code that runs the **create table** command to recreate the **orderdata** table, using the latest value for **thisTimeLastMonthInTicks** (any previous version of the table would have been created using an older value for this variable, so would reference data more than a month old):

    ```Python
    # Recreate the orderdata table using the new query

    spark.sql(command)
    ```

17. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.
18. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
19. In the new empty cell, enter the following code to create a Spark dataframe from the rows in the **orderdata** table, and display the first 20 rows:

    ```Python
    # Create a Spark dataframe using the records in the orderdata virtual table

    ordersdf = spark.sql("select * from orderdata")
    ordersdf.show()
    ```

20. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.

    > **Note**: You can view the progress of the Spark job that creates this dataset by expanding the **Spark Jobs** node under the cell as it runs. Click the (i) icon to see a detailed viewYou can also view the schema of the dataframe that is being created.

21. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
22. In the new empty cell, enter the following code that creates another Spark dataframe that summarizes the volume of sales and revenue generated for each product:

    ```Python
    # Create a dataframe that summarizes the data to show the total number of each product ordered, and the revenue generated by each product

    orderitemssummary = spark.sql("SELECT productnumber, productname, SUM(numberincartorordered) AS numordered, SUM(lineitemtotalcost) AS revenue FROM orderdata WHERE orderstatus <> 'Cancelled' GROUP BY productnumber, productname")
    orderitemssummary.show()
    ```

23. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.
24. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
25. In the new empty cell, enter the following code to create a further dataframe that sorts the data by volume ordered and displays the results:

    ```Python
    # Sort the data by volume ordered for each product, and show the results in a bar chart

    productsByVolume = orderitemssummary.orderBy("numordered", ascending = False)
    display(productsByVolume)
    ```

26. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**. The results should appear in a tabular format, with the most popular product at the top.
27. Below the table, click the graph drop-down arrow,  click **Bar**, and then click **Plot Options**.
28. In the **Customize Plot** dialog box, remove the fields from the **Keys** and **Values** boxes.
29. Drag the **productname** field to the **Series groupings** box, drag the **numberordered** field to the **Values** box, set **Aggregation** to **SUM**, and then click **Apply**. The bar chart should be displayed in the notebook.
30. On the toolbar, click the down-arrow icon, and then click **Add Cell Below**.
31. In the new empty cell, enter the following code to create a final dataframe that sorts the data by revenue and displays the results:

    ```Python
    # Sort the data by revenue for each product, and show the results in another bar chart

    productsByRevenue = orderitemssummary.orderBy("revenue", ascending = False)
    display(productsByRevenue)
    ```

32. On the toolbar, at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**.
33. Below the table, click the graph drop-down arrow,  click **Bar**, and then click **Plot Options**.
34. In the **Customize Plot** dialog box, remove the fields from the **Keys** box (leave the **Values** box set to **revenue**).
35. Drag the **productname** field to the **Series groupings** box, set **Aggregation** to **SUM**, and then click **Apply**.

### Task 4: Create a Dashboard

1. Return to the cell showing the bar chart that sorts orders by product volume.
2. On the toolbar, at the top right-hand side of the cell, click the **Show in Dashboard Menu** icon (this icon has the image of a graph), and then click **Add to New Dashboard**.
3. If the message box appears, click **Options for this site**, and then click **Always allow**.
4. On the **Analyze Orders** blade, change the title to **Monthly Orders Analysis**.
5. Resize the graph so that it occupies the entire presentation pane.
6. Under the dashboard title, adjacent to **View of notebook**, click the **Analyze Orders** link to return to the notebook.
7. Go to the final cell in the notebook, click the **Show in Dashboard** icon, select the **Monthly Orders Analysis** check box, and then click the **Go to Dashboard Monthly Orders Analysis** icon. You should see the revenue graph below the numberordered graph on the dashboard.
8. Resize the revenue graph to make it the same size as the numberordered graph.
9. Click **Run All** to test the notebook and dashboard. The data from the graphs will disappear while the notebook runs, and then reappear once the dataframes have been regenerated.
10. Close the Analyze Orders tab, but leave Internet Explorer open.
11. Close all remaining open windows.


### Task 5: Cleanup the lab environment

1. In the Azure portal, in the left pane, click **Resource groups**.
2. On the **Resource groups** blade, right-click **20777_Mod06**, and then click **Delete resource group**.
3. On the **Are you sure you want to delete "20777_Mod06"?** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **20777_Mod06**, and then click **Delete**.

---

© 2019 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
