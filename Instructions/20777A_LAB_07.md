# Module 7: Implementing Stream Processing with Cosmos DB

- [Module 7: Implementing Stream Processing with Cosmos DB](#module-7-implementing-stream-processing-with-cosmos-db)
  - [Lab: Using Cosmos DB with Stream Processing](#lab-using-cosmos-db-with-stream-processing)
    - [Lab Scenario](#lab-scenario)
    - [Objectives](#objectives)
    - [Lab Setup](#lab-setup)
  - [Exercise 1: Monitoring stock levels](#exercise-1-monitoring-stock-levels)
    - [Exercise 1 Scenario](#exercise-1-scenario)
    - [Task 1: Prepare the Environment](#task-1-prepare-the-environment)
    - [Task 2: Create a Cosmos DB collection for holding product sales statistics](#task-2-create-a-cosmos-db-collection-for-holding-product-sales-statistics)
    - [Task 3: Create an Azure Databricks account and cluster](#task-3-create-an-azure-databricks-account-and-cluster)
    - [Task 4: Create a Spark notebook for streaming orders data](#task-4-create-a-spark-notebook-for-streaming-orders-data)
    - [Task 5: Place orders and examine the streamed results](#task-5-place-orders-and-examine-the-streamed-results)
  - [Exercise 2: Handling Orders](#exercise-2-handling-orders)
    - [Exercise 2 Scenario](#exercise-2-scenario)
    - [Task 1: Create Azure ServiceBus queues](#task-1-create-azure-servicebus-queues)
    - [Task 2: Create an Azure Functions App account](#task-2-create-an-azure-functions-app-account)
    - [Task 3: Create the HandleOrders function](#task-3-create-the-handleorders-function)
    - [Task 4: Test the HandleOrders function](#task-4-test-the-handleorders-function)
    - [Task 5: Observe the query overhead of using the change feed](#task-5-observe-the-query-overhead-of-using-the-change-feed)
  - [Exercise 3: Displaying rolling revenue for a given time period](#exercise-3-displaying-rolling-revenue-for-a-given-time-period)
    - [Exercise 3 Scenario](#exercise-3-scenario)
    - [Task 1: Create a Databricks notebook for calculating rolling revenue](#task-1-create-a-databricks-notebook-for-calculating-rolling-revenue)
    - [Task 2: Place orders and observe the rolling revenue](#task-2-place-orders-and-observe-the-rolling-revenue)
  - [Exercise 4: Handling product deliveries](#exercise-4-handling-product-deliveries)
    - [Exercise 4 Scenario](#exercise-4-scenario)
    - [Task 1: Create an Azure IoT Hub](#task-1-create-an-azure-iot-hub)
    - [Task 2: Create an Azure Stream Analytics Job](#task-2-create-an-azure-stream-analytics-job)
    - [Task 3: Create a Cosmos DB collection for holding the details of product deliveries](#task-3-create-a-cosmos-db-collection-for-holding-the-details-of-product-deliveries)
    - [Task 4: Connect the IoT hub, Stream Analytics job, and Cosmos DB database](#task-4-connect-the-iot-hub-stream-analytics-job-and-cosmos-db-database)
  - [Task 5: Create the ProcessDeliveries function](#task-5-create-the-processdeliveries-function)
    - [Task 6: Simulate deliveries arriving at the warehouse](#task-6-simulate-deliveries-arriving-at-the-warehouse)
    - [Task 7: Clean up the lab environment](#task-7-clean-up-the-lab-environment)

## Lab: Using Cosmos DB with Stream Processing

### Lab Scenario

You have been asked to implement additional processing for the Adventure-Works ecommerce system. This processing involves handling large volumes of requests that are arriving continually, and that require immediate processing. Examples include alerting the warehouse and arranging dispatch whenever a new order is placed, maintaining detailed stock analytics, such as how the stock level for each product is varying, and whether new stock should be ordered.

### Objectives

At the end of this lab, you should be able to:

1. Use the Cosmos DB Change Feed to process changes to documents.
2. Create an Azure Function App that is triggered by changes made to documents in a Cosmos DB database
3. Stream data from a Cosmos DB database for processing externally.
4. Handle data received from an input stream.

### Lab Setup

  - **Estimated time**: 75 minutes
  - **Virtual machine**: 20777A-LON-DEV
  - **User name**: LON-DEV\\Administrator
  - **Password**: Pa55w.rd

## Exercise 1: Monitoring stock levels

### Exercise 1 Scenario

Customers are continually placing orders, and the stock levels of items in the warehouse are fluctuating continuously as customers make purchases and the stock is replenished by deliveries from the manufacturing plant. You need to monitor the sales of each product, to prevent the warehouse from holding excessive amounts of items that sell rarely, but also to maintain levels for products that sell well and minimize the number of back orders that can result when a product sells out. You decide to generate summary statistics for each product as orders are placed. These statistics will capture the volume of a product that has been sold during a given interval, and also the number that have been placed on backorder. You will save these statistics to a seperate collection in Cosmos DB. Using this information, the warehouse managers can spot trends in the sales of each product and make the necessary arrangements to ensure that sufficient stock is going to be availabl to satisfy future demand, either by increasing the manufacturing volume of a product, or by quickly ordering more in from a third party.

The main tasks for this exercise are as follows:

1. Prepare the environment.
2. Create a Cosmos DB collection for holding product sales statistics.
3. Create an Azure Databricks account and cluster.
4. Create a Spark notebook for streaming orders data.
5. Place orders and examine the streamed results.

### Task 1: Prepare the Environment

1. Ensure that the **MT17B-WS2016-NAT** and **20777A-LON-DEV** virtual machines are running, and then log on to **20777A-LON-DEV** as **LON-DEV\\Administrator** with the password **Pa55w.rd**.
2. In File Explorer, navigate to **E:\\Labfiles\\Lab07\\Starter**, right-click **full-cosmos-setup.ps1.txt**, and then click **Edit**.
3. In Notepad, edit **20777-mod7-sql-\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example **20777-mod7-sql-pjs14**.
4. Edit **20777blobmod7\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example, **20777blobmod7pjs14**.
5. Edit the **resourceGroupLocation** variable to specify the name of your nearest location.
6. On the **File** menu, click **Save**, and then clock Notepad.
7. In File Explorer, right-click **Setup.cmd**, and then click **Run as administrator**.
8. When prompted enter your Azure credentials.
9. When the script completes, press any key to close the command window.
10. On the toolbar, click **Internet Explorer**.
11. In Internet Explorer, go to **http://portal.azure.com**, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
12. In the Azure portal, in the left panel, click **All resources**, and then click **20777-mod7-sql-\<*your initials and day*\>**.
13. On the **20777-mod7-sql-\<*your initials and day*\>** blade, under **Settings**, click **Keys**.
14. Make a note of the **URI**, and **PRIMARY KEY** values.
15. In the left panel, click **All resources**, and then click **20777blobmod7\<*your initials and day*\>**.
16. On the **20777blobmod7\<*your initials and day*\>** blade, under **Settings**, click **Access keys**.
17. Make a note of the **Connection string** under **key1**.
18. On the Windows Start menu, click **Visual Studio 2017**.
19. On the **File** menu, point to **Open**, and then click **Project/Solution**.
20. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\MigrateProductData**, click **MigrateProductData.sln**, and then click **Open**.
21. In Solution Explorer, click **App.config**.
22. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
23. In the **BlobStorageConnectionString** setting, replace **\~CONNECTION STRING\~** with the **key1 Connection string** you noted earlier for the **20777blobmod7\<*your initials and day*\>** blob storage account.
24. Press F5 to build and run the application. 
25. In the console window, wait until the application has completed, and the close the application.
26. Close Visual Studio.

### Task 2: Create a Cosmos DB collection for holding product sales statistics

- Using the Azure portal, add another collection to the **Adventure-Works** database in your Cosmos DB account. Name the collection **ProductSales**, and partition it by the **/productnumber** field.

### Task 3: Create an Azure Databricks account and cluster

1. Using the Azure portal, create a new **Azure Databricks** account. Set the **Workspace name** to **20777a-databricks-\<*your name\>-\<the day*\>**, create the account in the **20777Mod07** resource group, set the **Location** to **West US 2**, and set the **Pricing Tier** to **Trial**.

    > **Note**: Currently, not all regions support the full range of VMs used by Azure Databricks to host Spark clusters, **West US 2** does.

2. When the account has been created and the service is deployed, go to your new Databricks account and click **Launch Workspace**.
3. Using the **Azure Databricks** page, create a new cluster named **\<*your name*\>-cluster**. Set **Max Workers** to **4**, but leave all other settings at their default values.
4. Wait while the cluster is created and started. Verify that its **State** is set to **Running** before continuing.

### Task 4: Create a Spark notebook for streaming orders data

> **Note**: The Scala code used in this task can be found in the file **E:\\Labfiles\\Lab07\\Solution\\Exercise01\\monitorstocklevels.txt**.

1. In the left toolbar, click **Azure Databricks**.
2. Click **Import Library**, and use the **New Library** page to create a new library. Name the library **Cosmos DB Connector** and upload the JAR file **azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar** from the **E:\Labfiles\Lab07\Starter\Exercise01** folder.
3. When the library has been created, attach the library to your cluster.
4. When the library has bee attached, in the toolbar on the left of the blade, click **Azure Databricks**.
5. Create a new Scala notebook named **monitorstocklevels**.
6. In the first cell of the notebook, enter the following code. This code specifies the modules that will be used by the notebook:

    ```Scala
    // Import libraries required for streaming, and for the Cosmos DB connector

    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    import com.microsoft.azure.cosmosdb.spark.config.Config
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark.streaming._
    import org.apache.spark.sql.functions._
    import java.sql.Timestamp
    ```

7. Add another cell, and enter the following code in the new cell. Replace **--URI--** with the **URI**, and replace **--KEY--** with the **PRIMARY KEY** values that you noted earlier for the **20777a-mod7-sql-\<*your name\>-\<the day*\>** Cosmos DB account. This code provides the parameters for connecting to the change feed of the **Data** collection in the **Adventure-Works** database. Notice that the **changefeedstartfromthebeginning** setting is false, so only new changes will be read. The schema sampling parameters enable the Cosmos DB Connector to infer the schema of the data in the change feed from a representative number of records; remember that the Cosmos DB collection contains many different types of documents, and each type has differet fields. It is important that spark is able to recognize all of the fields in each type of document.

    ``` Scala
    // Configure the connection to the Cosmos DB database

    val databaseReadConfig = Map(
        "endpoint" -> "--URI--",
        "masterkey" -> "--KEY--",
        "database" -> "Adventure-Works",
        "collection" -> "Data",
        "readchangefeed" -> "true",
        "changefeedqueryname" -> "ordersquery",
        "changefeedstartfromthebeginning" -> "false",
        "samplingratio" -> "1.0",
        "schema_samplesize" -> "1000",
        "changefeedcheckpointlocation" -> "/checkpointlocation"
    )
    ```

8. Add another cell to the notebook below the existing cell. Enter the following code in the new cell. This code starts reading the data from the change feed.

    ```Scala
    // Specify that spark should stream data from the change feed

    val changeFeedStream = spark
        .readStream
        .format(classOf[CosmosDBSourceProvider].getName)
        .options(databaseReadConfig)
        .load()
    ```

9. Add another cell, and enter the following code in the new cell. This code removes any documents that are not new orders from the change feed.

    ```Scala
    // The change feed contains documents of many types
    // Filter out all documents that are not new orders

    val ordersStream = changeFeedStream.filter("doctype == 'ShoppingCartOrder' AND isshoppingcartororder == 'Order' AND orderstatus == 'In progress'")
    ```
10. Add another cell, and enter the following code in the new cell. The data that you need is in the **orderitems** array in each order document (a single order can contain many orderitems). This code isolates the array, and discards the other data. The resulting simplified dataframe contains a single row for each orderitem record.

    ```Scala
    // Isolate the orderitems data, an array containing the number of items ordered for each product in the order, from the rest of the data

    val orderitemsFrame = ordersStream.select(explode($"orderitems").as("orderitems"))
    ```

11. Add another cell, and enter the following code in the new cell. This statement adds the current date and time as a column named **now** to each record in the stream.

    ```Scala
    // Add the current timestamp to the data frame.
    // This will be used for time watermarking and windowing

    val orderitemsFrameWithTimestamp = orderitemsFrame.withColumn("now", current_timestamp)
    ```

12. Add another cell, and enter the following code in the new cell. This code groups the data by product name (and number), and then calculates the volume sold and the number on backorder for that product across the records in the stream. The calculations are performed using a 30 second window, with a 15 second watermark to allow for latency (some orders might take their time reaching the stream, depending on the location of the customer).

    ```Scala
    // Calculate the number sold and number placed on backorder for products in all orders placed in the last 30 seconds

    val productsSoldFrame = orderitemsFrameWithTimestamp
        .withWatermark("now", "15 seconds")
        .groupBy($"orderitems.productname", $"orderitems.productnumber", window($"now", "30 seconds").as("timeslot"))
        .agg(sum($"orderitems.numberincartorordered").as("numberordered"), sum($"orderitems.numberonbackorder").as("numberonbackorder"))
    ```

13. Add another cell, and enter the following code in the new cell. Replace **--URI--** and **--KEY--** with the values for your Cosmos DB account. This code starts streaming the data containing the agggrates back to the Cosmos DB database and stores the data in the **ProductSales** collection in the **Adventure-Works** database.

    ```Scala
    // Stream the results back to Cosmos DB

    val databaseWriteConfig = Map(
        "endpoint" -> "--URI--",
        "masterkey" -> "--KEY--",
        "database" -> "Adventure-Works",
        "collection" -> "ProductSales",
        "checkpointLocation" -> "/checkpointwritelocation" // Note the capital "L" in "checkpointLocation"
    )

    val results = productsSoldFrame
        .select($"productnumber", $"productname", $"numberordered", $"numberonbackorder", date_format($"timeslot.end", "HH:mm:ss").as("timeslot"))
        .writeStream
        .format(classOf[CosmosDBSinkProvider].getName)
        .outputMode("append")
        .options(databaseWriteConfig)
        .start()
    ```

### Task 5: Place orders and examine the streamed results

1. On the desktop, start Visual Studio, and open the **PlaceOrders** solution in the **E:\\Labfiles\\Lab07\\Starter\\Exercise01\\PlaceOrders** folder. This app simulates customers placing orders.
2. Edit the **App.config** file, and set the value of the **EndpointUrl** and **PrimaryKey** settings to the endpoint URI adn primary key you recorded earlier for your Cosmos DB account.
3. Build and run the application. You will see messages indicating that orders are being created and added to the database.
4. Allow the app to run for a minute, and then return to the Databricks notebook.
5. Run the notebook and wait for the final cell in the notebook to indicate that it has started streaming data; the cell should run continuously, and you will see a **Last updated** message appear at regular intervals.

    > **Note**: The cell that starts streaming the data from the change feed maight report the error **Failed to load class "org.slf4j.impl.StaticLoggerBinder"**. You can ignore this error.

6. In the Azure portal, go to your Cosmos DB account, and use **Data Explorer**, to view the documents in the **ProductSales** collection in the **Adventure-Works** database.
7. Click any document in the collection. It should contain data similar to that shown below. The **timeslot** field indicates the end of the 30 second period during which the data was captured. The **numberordered** value shows how many items of the specified product were ordered in that interval, and the **numberonbackorder** value indicates how many items were placed on backorder because there is insufficient stock in place to satisfy orders.

    ```JSON
    {
        "productnumber": "FR-M94B-42",
        "timeslot": "11:32:00",
        "productname": "HL Mountain Frame - Black, 42",
        "numberordered": 1,
        "numberonbackorder": 2,
        "id": "65ec316e-b973-49eb-aed8-ff4df0760872",
        "_rid": "T3ILALy68wAKAAAAAAAAAA==",
        "_self": "dbs/T3ILAA==/colls/T3ILALy68wA=/docs/T3ILALy68wAKAAAAAAAAAA==/",
        "_etag": "\"0200812d-0000-0000-0000-5b7bf85e0000\"",
        "_attachments": "attachments/",
        "_ts": 1534851166
    }
    ```

8. Create and run the following  SQL query. Replace **\<Product Number\>** with the product number from the document that you just browsed, for example **FR-M94B-42**. You will see sales summary documents for each 30 second period during which orders were placed for the product. If any periods are missing, then no orders were received for the item.

    ```SQL
    SELECT * FROM c WHERE c.productnumber = "<Product Number>"
    ```

9. Note the number of documents returned by the query, wait for a minute, and then run the query again. If more orders have been placed, then more documents should be created.
10. Stop the **PlacedOrders** app and close Visual Studio.
11. Return to the **monitorstocklevels** notebook running in Internet Explorer and stop it running.

## Exercise 2: Handling Orders

### Exercise 2 Scenario

As customers place orders, the warehouse needs to be alerted to arrange assignment and dispatch of the ordered items. Additionally, the details of the order must be passed to the customer orders department so that the order can be recorded and audited correctly. The warehousing and customer orders systems are autonomous operations. Both systems listen for incoming messages on an Azure ServiceBus queue, and then retrieve the order information for processing. You need to ensure that the details of orders are posted to these queues whenever an order is placed. You decide to do this by using an Azure Function App.

The main tasks for this exercise are as follows:

1. Create Azure ServiceBus queues.
2. Create an Azure Functions App account.
3. Create the HandleOrders function
4. Test the HandleOrders function.
5. Observe the query overhead of using the change feed.

### Task 1: Create Azure ServiceBus queues

1. In the Azure portal, in the left panel, click **+ Create a resource**.
2. On the **New** blade, in the search box, type **Azure ServiceBus**, and then press Enter.
3. On the **Everything** blade, click **Service Bus**, and then click **Create**.
4. On the **Create namespace** blade, in the **Name** box, type **sb-20777-\<*your name\>-\<the day*\>**, for example, **sb-20777-john-31**.
5. In the **Pricing tier** drop-down list box, click **Basic**.
6. Under **Resource group**, in the drop-down list, click **20777Mod07**.
7. In the **Location** box, select a location near you, and then click **Create**.
8. In the Azure portal, in the left panel, click **All resources**, and then click **sb-20777-\<*your name\>-\<the day*\>**.
9. On the **sb-20777-\<*your name\>-\<the day*\>** blade, click **+ Queue**.
10. On the **Create queue** blade, in the **Name** box, type **customerordersqueue**, leave the remaining fields at their default values, and then click **Create**.
11. On the **sb-20777-\<*your name\>-\<the day*\>** blade, click **+ Queue**.
12. On the **Create queue** blade, in the **Name** box, type **warehousequeue**, leave the remaining fields at their default values, and then click **Create**.
13. On the **sb-20777-\<*your name\>-\<the day*\>** blade, under **Settings**, click **Shared access policies**.
14. On the **Shared access policies** blade, click the **RootManageSharedAccessKey** policy.
15. Make a note of the **Primary Connection String** value.

### Task 2: Create an Azure Functions App account

1. In the left panel, click **+ Create a resource**.
2. In the **New** blade, in the search box, type **Function App**, and then press Enter.
3. On the **Everything** blade, click **Function App**, and then click **Create**.
4. On the **Function App** blade, in the **App name** box, type **20777-func-\<*your name\>-\<the day*\>**, for example, **20777-func-john-31**.
5. Under **Resource Group**, click **Use existing**, then in the drop-down list, click **20777Mod07**.
6. In the **Location** box, select a location near you; you should use the same location that you used for your Cosmos DB account if it appears in the list.
7. Under **Application Insights**, click **Off**.
8. Leave all other settings at their default values, and then click **Create**.

### Task 3: Create the HandleOrders function

> **Note**: The complete code for the **HandleOrders** function is available in the file **HandleOrders.txt** in the **E:\\Labfiles\\Lab07\\Solution\\Exercise02** folder.

1. In the Azure portal, go to the Azure Functions App account that you created in the previous task and create a new function using the **Cosmos DB trigger** template.

    > **Note**: If the **Extensions not Installed** pane appears, click **Install** to install the template dependencies required to create Cosmos DB triggers, and then click **Continue** when installation is complete.

2. Configure the function app with the following settings:

    - **Name**: HandleOrders.
    - **Azure Cosmos DB account connection**: click **new**, and connect to your Cosmos DB account.
    - **Collection name**: Data.
    - **Create lease collection if it does not exist**: checked.
    - **Database name**: Adventure-Works.
    - **Collection name for leases**:  leases.

3. Use the **Integrate** pane to add the following Azure Service Bus output:

    > **Note**: If you are prompted to install the **Microsoft.Azure.WebJobs.Extensions.ServiceBus** extensions, click **Install**.

    - **Message type**: Service Bus Queue.
    - **Message parameter name**: customerOrdersQueue.
    - **Service Bus connection**:  click **new**, and connect to the **sb-20777-\<*your name\>-\<the day*\>** namespace.
    - **Queue name**: customerordersqueue.

4. Add another Service Bus output with the following configuration:

    - **Message type**: Service Bus Queue.
    - **Message parameter name**: warehouseQueue.
    - **Service Bus connection**:  use the default connection (this should be the one that you created for the previous Service Bus output).
    - **Queue name**: warehousequeue.

5. In the **Function Apps** pane, under the **HandleOrders**.
6. In the blade showing the **run.csx** script, add a reference to the **Newtonsoft.Json** assembly at the top of the file, after the reference to the **Microsoft.Azure.DocumentDB.Core** assembly.
7. Add a **Using** directive that brings the **Newtonsoft.Json** namespace into scope.
8. After the closing brace of the **Run** function, add the following class definitions to the file. These classes define the structure of the orders and order items that the Advnture-Works web app writes to the Cosmos DB database.

    ```CSharp
    // Base class for all root document types
    public abstract class DocumentType
    {
        [JsonProperty("doctype")]
        public string DocType { get; set; }

        [JsonProperty("ttl")]
        public int TimeToLive { get; set; } = -1;

        public DocumentType()
        {
            this.DocType = this.GetType().Name;
        }
    }

    public class OrderItem
    {
        [JsonProperty("productnumber")]
        public string ProductNumber { get; set; }

        [JsonProperty("productname")]
        public string ProductName { get; set; }

        [JsonProperty("productid")]
        public string ProductID { get; set; }

        [JsonProperty("subcategory")]
        public string Subcategory { get; set; }

        [JsonProperty("numberincartorordered")]
        public int NumberInCartOrOrdered { get; set; }

        [JsonProperty("numberonbackorder")]
        public int NumberOnBackorder { get; set; }

        [JsonProperty("backorderreference")]
        public string BackorderReference { get; set; }

        [JsonProperty("unitcost")]
        public decimal UnitCost { get; set; }

        [JsonProperty("lineitemtotalcost")]
        public decimal LineItemTotalCost { get; set; }
    }

    public class ShoppingCartOrder : DocumentType
    {
        [JsonProperty("id")]
        public string ShoppingCartOrderID { get; set; }

        [JsonProperty("partitionkey")]
        public string CustomerID { get; set; }

        [JsonProperty("isshoppingcartororder")]
        public string IsShoppingCartOrOrder { get; set; }

        [JsonProperty("orderitems")]
        public List<OrderItem> OrderItems { get; set; }

        [JsonProperty("numberofitems")]
        public int NumberOfItems { get; set; }

        [JsonProperty("itemscost")]
        public decimal ItemsCost { get; set; }

        [JsonProperty("customerdiscountrate")]
        public int CustomerDiscountRate { get; set; }

        [JsonProperty("totalcost")]
        public decimal TotalCost { get; set; }

        [JsonProperty("dateplaced")]
        public long DatePlaced { get; set; }

        [JsonProperty("orderstatus")]
        public string OrderStatus { get; set; } // "In progress", "Delivered", "Cancelled"

        [JsonProperty("lastupdated")]
        public long LastUpdated { get; set; }
    }
    ```

9. Modify the definition of the **Run** method and add the two output parameters for the Service Bus queues, as follows:

    ```CSharp
    public static void Run(IReadOnlyList<Document> input, ILogger log, ICollector<string> customerOrdersQueue, ICollector<string> warehouseQueue)
    ```
    
    > **Note**: The Service Bus Webjobs extension abstracts the message queues as **ICollector** objects. YOu post a message to the queue simply by using the **Add** method of the **ICollector** interface.
    
10. Remove the two **log.LogInformation**statements from the body of the function.
11. Inside the **if** statement, add code that implements the following logic:
    - Iterate through the documents in the change feed.

        ```CSharp
        foreach (Document doc in input)
        {
            // Code that handles each document goes here
        }
        ```

    - For each document in the change feed, perform the following tasks:
        - Deserialize it from the JSON format use dby the change feed and convert it into a ShoppingCartOrder object, as follows:

            ```CSharp
            ShoppingCartOrder orderDoc = JsonConvert.DeserializeObject<ShoppingCartOrder>(doc.ToString());
            ```

        - Verify that it is an order that is "In progress" (as opposed to cancelled or delivered), or some other type of document:

            ```CSharp
            if (orderDoc.DocType == "ShoppingCartOrder" && orderDoc.IsShoppingCartOrOrder == "Order" && orderDoc.OrderStatus == "In progress")
            {
                // Code for the next two bullets goes in here
            }
            ```
        - Alert the warehouse with the details of the items in the order
  
            ```CSharp
            foreach (OrderItem item in orderDoc.OrderItems)
            {
                log.LogInformation($"New order for {item.NumberInCartOrOrdered} of {item.ProductNumber} ({item.ProductName})");
                warehouseQueue.Add(JsonConvert.SerializeObject(item));
            }
            ```

        - Send a message to the customer orders department containing the details of the order.

            ```CSharp
            log.LogInformation($"New order, {orderDoc.ShoppingCartOrderID}, for customer {orderDoc.CustomerID}. Value is {orderDoc.TotalCost}");
            customerOrdersQueue.Add(JsonConvert.SerializeObject(orderDoc));
            ```

12. Save the updated function.

### Task 4: Test the HandleOrders function

1. Open the blade that displays the log trace for the HandleOrders function.
2. Return to the desktop, and using Visual Studio, open the **Adventure-Works** solution in the **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\Adventure-Works** folder.
3. Edit the **Web.config** file and set the **EndpointUrl** and **PrimaryKey** settings with the corresponding  values for your Cosmos DB account.
4. Build and run the application.
5. Click **Search By Category**, click **Details** for the first item returned by the search, and then click **Add to Shopping Cart**. Log in as customer **1** with no password when prompted. In the **Item added to shopping cart** message box, click **OK**. Do not click **Place Order** just yet.
6. Switch back to the Azure portal, showing the log trace for the **HandleOrders** function. Verify that it displays messages similar to these.

    ```Text
    2018-10-10T17:00:41.294 [Information] Executing 'Functions.HandlerOrders' (Reason='New changes on collection Data at 2018-10-10T17:00:41.2930445Z', Id=94d0a322-dc7b-493c-b96b-557502f9a01c)
    2018-10-10T17:00:41.297 [Information] Executed 'Functions.HandlerOrders' (Succeeded, Id=94d0a322-dc7b-493c-b96b-557502f9a01c)
    ```

7. Return to the Adventure-Works web site, and then click **Place Order**. In the **Order placed** message box, click **OK**.
8. Switch back to the Azure portal. In the log for the Azure function, you should now see messages similar to these included in the output.

    ```Text
    2018-10-10T17:04:26.243 [Information] New order for 1 of PD-M282 (LL Mountain Pedal)
    2018-10-10T17:04:29.113 [Information] New order, 92ba4d40-a970-48e4-a98d-bad85edacb38, for customer 1. Value is 36.441
    ```

9. Return to the desktop, start another instance of Visual Studio, and open the **CustomerOrders** solution in the **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\CustomerOrders** folder. This app monitors the customerordersqueue ServiceBus queue.
10. Edit the **App.config** file, and set the value of the **ServiceBusConnectionString** setting to the connection string you recorded earlier for the Service Bus queue.
11. Build and run the application. You should see a message appear that describes the order you just placed using the Adventure-Works web site (if you placed more than one order, you will see more than one message). This message was posted by the **HandleOrders** Azure Function App.
12. Leave the app running, start a third instance of Visual Studio, and open the **Warehouse** solution in the **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\Warehouse** folder. This app monitors the warehousequeue ServiceBus queue.
13. In Solution Explorer, edit the **App.config** file, and set the value of the **ServiceBusConnectionString** setting to the connection string you recorded earlier for the Service Bus queue.
14. Build and run the application. You should see a messages appear that describe the items in the order you just placed. These messages were also posted by the **HandleOrders** Azure Function App.
15. Return to the Adventure-Works web site, and experiment with placing more orders. Notice how the details of the orders are processed by the HandleOrders function app, and the results displayed by the CustomerOrders and Warehouse apps.
16. When you have finished, close the Aventure-Works web site, but leave the CustomerOrders and Warehouse apps running.

### Task 5: Observe the query overhead of using the change feed

1. In the Azure portal, go to the blade for your Cosmos DB account, and under **Monitoring**, click **Metrics**.
2. Display the **Throughput** summary for the **Data** collection in the **Adventure-Works** database.
3. Return to the desktop, start a further instance of Visual Studio, and open the **PlaceOrders** solution in the **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\PlaceOrders** folder. This is another version of the app that simulates customers placing orders.
4. Edit the **App.config** file, and  set the value of the **EndpointUrl** and **PrimaryKey** settings to the endpoint URI and primary key for your Cosmos DB account.
5. Build and run the application. YOu will see messages indicating that orders are being created and added to the database.
6. Switch to the window displaying the ouput of the CustomerOrders app, and verify that you see new orders being processed.
7. Wait for 5 or 10 minutes while the PlaceOrders generates orders.

    > **Note**: The CustomerOrders app might display an exception ocassionally when it finds an unexpected character appearing in a message retrieved from the queue. You can ignore these exceptions.

8. Leave the PlaceOrders app running, and return to the Azure portal displaying the throughput for the Cosmos DB database. Click **Refresh** to see the most recent statistics. Note that the updated statistics will not appear immediately, so you might need to wait for a couple more minutes and then click **Refresh** again.
9. Make a note of the number of requests aggregated over a 1 minute interval.
10. Go to the blade for your function app and stop the app running the HandleOrders function.
11. Switch back to the metrics for your Cosmos DB, and display the throughput for the Data collection in the Adventure-Works database.
12. Wait for a few minutes, periodically clicking **Refresh**. You should see that the number of requests aggregated over a 1 minute interval drops to less than half the previous value (the PlaceOrders app should still be running, but the CustomerOrders and Warehouse apps will appear to have stalled as no more messages are appearing on the ServiceBus queues). Although the function app was not performing any explicit queries, listening to the change feed adds significantly to the workload on the database. You should bear this overhead in mind when building your own applications that use the change feed of a collection.
13. Stop the Warehouse, CustomerOrders, and PlaceOrders apps, and close all instances of Visual Studio.

## Exercise 3: Displaying rolling revenue for a given time period

### Exercise 3 Scenario

You want to see how revenue varies from day to day, and week to week. You decide to implement a notebook that streams the orders data from the Cosmos DB database as new orders are placed, and uses a sliding time window to display the rolling revenue over various time intervals. To prototype this approach, you will use a 5 minute window with revenues calculated each minute. Once you are happy with the approach, you can adjust the window duration and time intervals to match your requirements.

> **Note**: Long duration time intervals, such as a week or a month, will require that the Databricks cluster is able to buffer the data for that time window in memory. If customers are placing 1000 orders a minute, and each order document occupies 1Kb, then this will require 1440Mb of memory per day across the cluster, just over 10Gb to hold the data for a week, or somewhere in excess of 40Gb for a month's worth. You should size your clusters appropriately when you create them to match the memory requirements of your workloads.

The main tasks for this exercise are as follows:

1. Create a Databricks notebook for calculating rollowing revenue.
2. Place orders and observe the rolling revenue.

### Task 1: Create a Databricks notebook for calculating rolling revenue

> **Note**: The Scala code used in this task can be found in the file **E:\\Labfiles\\Lab07\\Solution\\Exercise03\\rollingrevenue.txt**.

1. In the Azure portal, go to your Databricks service open the workspace.
2. Create a new Scala notebook using your existing Databricks cluster. Name the workbook **rollingrevenue**.
3. In the first cell of the notebook, enter the following code.

    > **Note**: The code for the first few cells in the notebook is the same as the earlier exercise. This is because the notebook streams data from the change feed of the same collection.

    ```Scala
    // Import libraries required for streaming, and for the Cosmos DB connector

    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    import com.microsoft.azure.cosmosdb.spark.config.Config
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark.streaming._
    import org.apache.spark.sql.functions._
    import java.sql.Timestamp
    ```

4. Add another cell to the worbook and enter the code shown below. Replace **--URI--** with the URI of your **20777a-mod7-sql-\<*your name\>-\<the day*\>** Cosmos DB account, and replace **--KEY--** with the primary access key for the account.

    ``` Scala
    // Configure the connection to the Cosmos DB database

    val databaseReadConfig = Map(
        "endpoint" -> "--URI--",
        "masterkey" -> "--KEY--",
        "database" -> "Adventure-Works",
        "collection" -> "Data",
        "readchangefeed" -> "true",
        "changefeedqueryname" -> "ordersquery",
        "changefeedstartfromthebeginning" -> "false",
        "samplingratio" -> "1.0",
        "schema_samplesize" -> "1000",
        "changefeedcheckpointlocation" -> "/checkpointlocation2"
    )
    ```

5. Add another cell and enter the following code.

    ```Scala
    // Specify that spark should stream data from the change feed

    val changeFeedStream = spark
        .readStream
        .format(classOf[CosmosDBSourceProvider].getName)
        .options(databaseReadConfig)
        .load()
    ```

6. Add another cell and enter the following code.

    ```Scala
    // Filter out all documents that are not new orders

    val ordersStream = changeFeedStream.filter("doctype == 'ShoppingCartOrder' AND isshoppingcartororder == 'Order' AND orderstatus == 'In progress'")
    ```

7. Add another cell and enter the following code. This code trims the dataframe to include only the totalcost and the itemscost fields from the order, together with the current time. The itemscost fields holds the cost of the order before any customer discounts are applied, and the totalcost field contains the value after applying the discounts.

    ```Scala
    // Extract the revenue details from the dataframe, and include the current time

    val revenueDetailsFrame = ordersStream
        .select($"totalcost", $"itemscost")
        .withColumn("now", current_timestamp)
    ```

8. Add another cell and enter the following code. This statement defines the sliding time window for the data frame. The first time (5 minutes) is the duration of the window, and the second time (1 minute) indicates the sliding period. The aggregations will be performed at 1 minute intervals across the preceeding 5 minutes of data.

    ```Scala
    // Calculate the rolling revenue for the 5 minutes, with a sliding window of 1 minute

    val revenuePerMinute = revenueDetailsFrame
        .withWatermark("now", "30 seconds")
        .groupBy(window($"now", "5 minutes", "1 minute").as("time"))
        .agg(sum($"totalcost").as("revenueperminute"), sum($"itemscost").as("revenueperminutebeforediscount"))
    ```

9. Add another cell and enter the following code. This code saves the result of the aggregations to a virtual table in memory. Note that you should only use this approach for relatively small sets of data (in this case, there will be one row added to the table each minute). For higher volumes, you should consider writing the results back to persistent storage, such as a Cosmos DB database, as shown in exercise 1.

    ```Scala
    // Capture the rolling revenue figures to a table in memory

    val query = revenuePerMinute
        .writeStream
        .format("memory")        // memory = store data in-memory in a table (only do this if the results of the streaming aggregation are low-volume)
        .queryName("revenue")    // revenue = name of the in-memory table
        .outputMode("append")  
        .start()
    ```

10. Add another cell and enter the following code. This code runs an SQL *magic* that retrieves the results from the table in memory.

    ```Scala
    %sql
    select revenueperminute, revenueperminutebeforediscount, revenueperminutebeforediscount - revenueperminute as discounted, time.end as time 
    from revenue
    ```

### Task 2: Place orders and observe the rolling revenue

1. Start Visual Studio, and open the **PlaceOrders** solution in the **E:\\Labfiles\\Lab07\\Starter\\Exercise03\\PlaceOrders** folder. This is an amended version of the app from the previous exercises; it clears down the existing orders in the Cosmos DB database before generating new ones.
2. Edit the **App.config** file, and set the value of the **EndpointUrl** and **PrimaryKey** settings to the endpoint URI and primary key you recorded earlier for your Cosmos DB account.
3. Build and run the application. The existing orders will be deleted from the database (to tidy things up), and then you will be prompted to press Enter to allow the app to start creating new orders.
4. Allow the app to run for a minute once it starts creating new orders, and then return to the Databricks notebook (leave the app running).
5. Run the notebook. Wait for the penultimate cell in the notebook to indicate that it has started streaming data; the cell should run continuously, and you will see a **Last updated** message appear at regular intervals.
6. Allow the notebook to run for another minute, and then in the toolbar at the top right-hand side of the final cell in the notebook, on the **Run cell** menu, click **Run cell**. A table displaying data for the last few minutes (1 row per minute) should appear. The figures in the **revenueperminute** and **revenueperminutebeforediscount** columns show the rolling revenue for the 5 minutes that end in the **time** column.
7. Below the table, click the drop-down list by the graph icon, and then click **Line (deprecated)**.
8. Click **Plot Options**.
9. In the **Customize Plot** dialog box, delete all columns from the **Keys**, **Series groupings**, and **Values** boxes.
10. Add the **time** column and to the **Keys** box.
11. Add the **revenueperminute**,  **revenueperminutebeforediscount**, and **discounted** columns to the **Values** box.
12. Click **Apply**. The table in the notebook should be replaced with a graph depicting the rolling revenue values.
13. Run the cell again. The graph should be updated with the latest values. 
14. Wait for another minute and run the cell again. Repeat this process for 5 minutes. At this point, the data in the graph should start to level off. This is because for the first four minutes, the number of orders in the database is starting from a low point (no orders). After 5 minutes, the number of orders in the preceding 5 minute window should be reasonably constant, and the rolling revenue should also tend to even out.
15. Stop the notebook, and then close the Azure Databricks portal.
16. Stop the **PlacedOrders** app and close Visual Studio.

## Exercise 4: Handling product deliveries

### Exercise 4 Scenario

The warehouse frequently receives deliveries of goods, either from the manufacturing plant or from other suppliers. Whenever a delivery occurs, the goods are unpacked and placed in the warehouse. Warehouse staff use handheld devices to scan the items as they are stored, and the data is sent to an IoT hub where the data can be retrieved and processed. You have been asked to ensure that the product levels recoded in the Cosmos DB database are kept up to date. You decide to use Azure Stream Analytics to capture the delivery data to a Cosmos DB collection, and an Azure Function App that is triggered by the arrival of new delivery data to update the stock levels in the Data collection in Adventure-Works database.

The main tasks for this exercise are as follows:

1. Create an Azure IoT Hub.
2. Create an Azure Stream Analytics Job.
3. Create a Cosmos DB collection for holding the details of product deliveries.
4. Connect the IoT hub, Stream Analytics job, and Cosmos DB database.
5. Create the ProcessDeliveries function.
6. Simulate deliveries arriving at the warehouse.
7. Clean up the lab environment.

### Task 1: Create an Azure IoT Hub

1. In the Azure portal, in the left panel, click **+ Create a Resource**.
2. On the **New** blade, in the search box, type **IoT**, and then press Enter.
3. On the **Everything** blade, click **IoT Hub**, and then click **Create**.
4. On the **IoT hub** blade, on the **Basics** tab, in the **Resource Group** drop-down list, click **20777Mod07**.
5. In the **Region** drop-down list, click the region closest to your current location.
6. In the **IoT Hub Name** box, type **20777a-iothub-\<*your name\>-\<the day*\>**, and then click **Size and Scale**.
7. On the **Size and Scale** tab, in the **Pricing and scale tier** drop-down list, click **B1: Basic Tier**. 
8. Leave the remaining settings at their default values, and then click **Review + create**.
9. On the **Review + create** tab, click **Create**.
10. Wait while the IoT hub is created and deployed.
11. In the Azure portal, in the left panel, click **All resources**, and then click **20777a-iothub-\<*your name\>-\<the day*\>**.
12. On the **20777a-iothub-\<*your name\>-\<the day*\>** blade, under **Settings**, click **Shared access policies**, and then click **device**.
13. Make a note of the **Connection string-primary key** value.
14. Close the **device** blade.
15. On the **20777a-iothub-\<*your name\>-\<the day*\>** blade, under **Explorers**, click **IoT devices**, and then click **+ Add**.
16. On the **Add Device** blade, in the **Device ID** box, type **Scanner**.
17. Leave the other settings as their default values, and then click **Save**.

### Task 2: Create an Azure Stream Analytics Job

1. In the left panel, click **+ Create a resource**.
2. On the **New** blade, in the search box, type **Stream**, and then press Enter.
3. On the **Everything** blade, click **Stream Analytics job**, and then click **Create**.
4. On the **New Stream Analytics job** blade, in the **Job name** box, type **20777a-job-\<*your name\>-\<the day*\>**.
5. In the **Resource group** drop-down list, click **20777Mod07**.
6. In the **Location** drop-down list, click the location closest to your current location.
7. Set **Streaming units** to **6**, and then click **Create**.
8. Wait while the stream analytics job is created.

### Task 3: Create a Cosmos DB collection for holding the details of product deliveries

1. In the Azure portal, in the left panel, click **All resources**, and then click **20777a-mod7-sql-\<*your name\>-\<the day*\>**.
2. On the **20777a-mod7-sql-\<*your name\>-\<the day*\>** blade, click **Data Explorer**, right-click **Adventure-Works**, and then click **New Collection**.
3. On the **Add Collection** blade, in the **Collection Id** box, type **Deliveries**.
4. Under **Storage Capacity**, click **Unlimited**.
5. In the **Partition key** box, type **/productnumber**, and then click **OK**.

### Task 4: Connect the IoT hub, Stream Analytics job, and Cosmos DB database

1. In the left panel, click **All resources**, and then click **20777a-job-\<*your name\>-\<the day*\>**.
2. On the **20777a-job-\<*your name\>-\<the day*\>** blade, under **Job topology**, click **Inputs**, click **+ Add stream input**, and then click **IoT Hub**.
3. On the **IoT Hub** blade, in the **Input alias** box, type **ScannerDevicesHub**.
4. In the **Shared access policy name** drop-down list, click **service**.
5. Leave the remaining fields at their default values, and then click **Save**.
6. On the **20777a-job-\<*your name\>-\<the day*\>** blade, under **Job topology**, click **Outputs**, click **+ Add**, and then click **Cosmos DB**.
7. In the **Cosmos DB** blade, in the **Output alias** box, type **AdventureWorks**.
8. Select **Provide Cosmos DB settings manually**, in the **Account id** box, type **20777a-mod7-sql-\<*your name\>-\<the day*\>**, and in the **Account Key** box, enter the primary key for your Cosmos DB account thet you recorded in the previous task.
9. Under **Database** click **Use existing** and type **Adventure-Works**.
10. In the **Collection name pattern** box type **Deliveries**.
11. In the **Document id** box type **id**, and then click **Save**.
12. Wait while the connection to the output is verified and validated.
13. On the **20777a-job-\<*your name\>-\<the day*\>** blade, under **Job topology**, click **Query**.
14. Specify the following query, and then click **Save**.

    ```SQL
    SELECT
        id, productnumber, volumedelivered, time
    INTO
        AdventureWorks
    FROM
        ScannerDevicesHub
    ```


## Task 5: Create the ProcessDeliveries function

> **Note**: The complete code for the **ProcessDeliveries** function is available in the file **ProcessDeliveries.txt** in the **E:\\Labfiles\\Lab07\\Solution\\Exercise04** folder.

1. In the Azure portal, go to the Azure Functions App account that you created in exercise 1, **20777-func-\<*your name\>-\<the day*\>**. Start the function app if it is not currently running.
2. Create a new Cosmos DB trigger function with the following settings:
    - **Name**: ProcessDelivieries
    - **Azure Cosmos DB trigger**: Accept the existing **Azure Cosmos DB account connection** configuration
    - **Collection name**: Deliveries.
    - **Create lease collection if it does not exist**: Checked.
    - **Database name**: Adventure-Works.
    - **Collection name for leases**: leases.
3. In the **Function Apps** pane, click **20777-func-\<*your name\>-\<the day*\>**, click **Overview**, and then click **Application settings**.
4. Scroll down to the **Application settings** section of the blade, click **+ Add new setting**.
5. Add the following settings and then click **Save**:
    - **APP SETTING NAME**: EndpointUrl, **VALUE**: The URI of your Cosmos DB account.
    - **APP SETTING NAME**: PrimaryKey, **VALUE**: The primary key of your Cosmos DB account.
    - **APP SETTING NAME**: Database, **VALUE**: Adventure-Works.
    - **APP SETTING NAME**: Collection, **VALUE**: Data.
6. In the **Function Apps** pane, click the **ProcessDeliveries** function.
7. In the **run.csx** blade, add a reference to the **Newtonsoft.Json** assembly at the top of the file, after the reference to the **Microsoft.Azure.DocumentDB.Core** assembly, as shown by the following code.

    ```CSharp
    #r "Newtonsoft.Json"
    ```

8. Add the following **Using** directives to those already in the script:

    ```CSharp
    using Microsoft.Azure.Documents.Client;
    using System.Net;
    using Newtonsoft.Json;
    ```

9. After the closing brace of the **Run** function, add the following class definitions to the file. The **ProductDeliveryRecord** class is the structure of delivery records that the warehouse scanner devices will send to the IoT hub. The remaining classes define the structure of product documents. Note that the DocType class has been modified slightly to inherit from the **Document** class. This enables you to access fields such as the **ETag** for a document.

    ```CSharp
    // Document types representing delivery records and products
    class ProductDeliveryRecord: Document
    {
        [JsonProperty("id")]
        public string ID { get; set; }

        [JsonProperty("productnumber")]
        public string ProductNumber { get; set; }

        [JsonProperty("volumedelivered")]
        public int Volume { get; set; }

        [JsonProperty("time")]
        public DateTime Time { get; set; }

        public override string ToString()
        {
            return $"Product: {ProductNumber}, Volume: {Volume}, Time: {Time}";
        }
    }

    public abstract class DocumentType: Document
    {
        [JsonProperty("doctype")]
        public string DocType { get; set; }

        public DocumentType()
        {
            this.DocType = this.GetType().Name;
        }
    }

    public class ProductCategoryData : DocumentType
    {
        [JsonProperty("partitionkey")]
        public string Subcategory { get; set; }

        [JsonProperty("category")]
        public string Category { get; set; }
    }

    public class ProductDocumentData
    {
        [JsonProperty("documenttitle")]
        public string DocumentTitle { get; set; }

        [JsonProperty("documentsummary")]
        public string DocumentSummary { get; set; }

        [JsonProperty("document")]
        public string Document { get; set; }
    }

    public class ProductImageData
    {
        [JsonProperty("diagram")]
        public string Diagram { get; set; }

        [JsonProperty("thumbnail")]
        public string Thumbnail { get; set; }

        [JsonProperty("largephoto")]
        public string LargePhoto { get; set; }
    }

    public class Product : DocumentType
    {
        [JsonProperty("id")]
        public string ProductID { get; set; }

        [JsonProperty("partitionkey")]
        public string Subcategory { get; set; }

        [JsonProperty("productcategory")]
        public string Category { get; set; }

        [JsonProperty("productname")]
        public string ProductName { get; set; }

        [JsonProperty("productnumber")]
        public string ProductNumber { get; set; }

        [JsonProperty("color")]
        public string Color { get; set; }

        [JsonProperty("listprice")]
        public decimal ListPrice { get; set; }

        [JsonProperty("size")]
        public string Size { get; set; }

        [JsonProperty("weight")]
        public string Weight { get; set; }

        [JsonProperty("quantityinstock")]
        public int QuantityInStock { get; set; }

        [JsonProperty("model")]
        public string Model { get; set; }

        [JsonProperty("description")]
        public string Description { get; set; }

        [JsonProperty("documentation")]
        public ProductDocumentData Documentation { get; set; }

        [JsonProperty("images")]
        public ProductImageData Images { get; set; }
    }
    ```

10. Modify the definition of the **Run** method to indicate that it supports asynchronous operations.

    ```CSharp
    public static async Task Run(IReadOnlyList<Document> input, ILogger log)
    ```

11. Remove the following two statements from the body of the function:

    ```CSharp
    log.LogInformation("Documents modified " + input.Count);
    log.LogInformation("First document Id " + input[0].Id);
    ```

12. Inside the **if** statement, add the following lines of code that retrieve the configuration settings required to connect to the Data collection in the Adventure-Works database, and then use these settings to create a DocumentClient object.

    > **Note**: Application settings are exposed as environment variables rather than configuration settings.

    ```CSharp
    string endpointUrl = System.Environment.GetEnvironmentVariable("EndpointUrl", EnvironmentVariableTarget.Process);
    string primaryKey = System.Environment.GetEnvironmentVariable("PrimaryKey", EnvironmentVariableTarget.Process);
    string database = System.Environment.GetEnvironmentVariable("Database", EnvironmentVariableTarget.Process);
    string collection = System.Environment.GetEnvironmentVariable("Collection", EnvironmentVariableTarget.Process);

    var client = new DocumentClient(new Uri(endpointUrl), primaryKey);
    ```

13. Inside the **if** statement, add the following statements that loop through the soduments appearing in the change feed.

    ```CSharp
    // Iterate through the change feed
    foreach (Document doc in input)
    {

    }
    ```

14. Inside the **foreach** block, add the following code:

    ```CSharp
    // Deserialize each document - it should be a ProductDeliveryRecord
    ProductDeliveryRecord deliveryDoc = JsonConvert.DeserializeObject<ProductDeliveryRecord>(doc.ToString());

    // Display the details on the trace log
    log.LogInformation(deliveryDoc.ToString());
    ```
15. Still inside the **foreach** block, immediately after the code you have just entered, add the following statements:

    ```CSharp
    // Update the volume in stock in the Data collection in the Adventure-Works database
    var dataCollectionUri = UriFactory.CreateDocumentCollectionUri(database, collection);
    var productQueryText = "SELECT * FROM c WHERE c.doctype = 'Product' AND c.productnumber = @productnum";
    Product productDoc;
    ```

16. Add the following code after the statements that you entered in the previous step. This code runs the query that fetches the corresponding product from the **Data** collection, and then updates the **QuantityInStock** field with the value from the new delivery. The **SaveDocument** function attempts to write the updated product document back to the **Data** collection. The **SaveDocument** returns a boolean that indicates whether the save operation was successful; while customers are placing orders, the stock levels are updated, and it is important not to lose these changes. The **SaveDocument** method uses ETags to prevent this situation from occuring. The *do* loop iterates, repeatedly fetching the latest version of the product document and attempting to save it until the operation is successful.

    ```CSharp
    do
    {
        // Find the product in the Data collection
        var productParam = new SqlParameterCollection { 
            new SqlParameter("@productnum", deliveryDoc.ProductNumber)
        };

        var productQuery = client.CreateDocumentQuery(dataCollectionUri, new SqlQuerySpec
        {
            QueryText = productQueryText,
            Parameters = productParam
        }, new FeedOptions
        {
            EnableCrossPartitionQuery = true
        }).AsEnumerable();

        productDoc = productQuery.Take(1).Single();
        log.LogInformation($"product is {productDoc.ProductName}"); 

        // Update the quantityinstock field of the product doc to reflect the new delivery
        productDoc.QuantityInStock += deliveryDoc.Volume;
    }
    // Save the updated product doc back to the database.
    // If the Save operation fails, it could be due to a conflict caused by a simultaneous update such as a customer placing an order.
    // If so, try querying the product to get the most up to date information and save it again
    while(! await SaveDocument(productDoc, client, log));
    ```

17. After the closing brace of the **Run** method, but above the definition of the **ProductDeliveryRecord** class, add the following function. This function saves the document, using the ETag to ensure that the document has not been modified by another user since it was retrieved.

    ```CSharp
    // Save a document and check for conflicts by using the ETag
    // Return true if the update was successful, false otherwise
    private static async Task<bool> SaveDocument(Document doc, DocumentClient client, ILogger log)
    {
        try
        {
            // Replace the document in the database with the updated doc
            RequestOptions options = new RequestOptions
            {
                AccessCondition = new AccessCondition
                {
                    Condition = doc.ETag,
                    Type = AccessConditionType.IfMatch
                }
            };

            var response = await client.ReplaceDocumentAsync(doc.SelfLink, doc, options);
            log.LogInformation($"Document replaced. Status code is {response.StatusCode}");
            return true;
        }
        catch (DocumentClientException dce)
        {
            // If a conflict occured, display a message
            if (dce.StatusCode == HttpStatusCode.PreconditionFailed)
            {
                log.LogInformation("Update failed - another user already changed this document. Requery and try again");
            }
            return false;
        }
    }
    ```

18. Click **Save**.

### Task 6: Simulate deliveries arriving at the warehouse

1. In the Azure portal, go to the Azure Stream Analytics job you created in Task 2, **20777a-job-\<*your name\>-\<the day*\>**.
2. On the **20777a-job-\<*your name\>-\<the day*\>** blade, click **Overview**.
3. Click **Start**. In the **Start job** blade, click **Start**. Wait for the stream analytics job to start before continuing.
4. Return to the desktop, and using Visual Studio, open the **ProductScanner** solution in the **E:\\Labfiles\\Lab07\\Starter\\Exercise04\\ProductScanner** folder.
5. Edit the **App.config** file, and in the **appSettings** section, set the **DeviceConnectionString** settings to the **Connection string-primary key** for your IoT hub, that you recorded in Task 1.
6. Build and run the application. You should see messages appearing at 5 second intervals representing new deliveries.
7. Return to the Azure portal, and then go to your Cosmos DB account, **20777a-mod7-sql-\<*your name\>-\<the day*\>**.
8. Using **Data Explorer**, examine the documents in the **Deliveries** collection. These documents have been added by the Azure Stream Analytics job, using the data streamed from the IoT Hub.
9. Click any document and examine its contents. It should look similar to this:

    ```JSON
    {
       "id": "24037fa0-06b9-404a-912d-017464462f63",
        "productnumber": "TO-2301",
        "volumedelivered": 99,
        "time": "2018-08-22T12:28:07.2210179Z",
        "_rid": "T3ILALIG40ATAAAAAAAAAA==",
        "_self": "dbs/T3ILAA==/colls/T3ILALIG40A=/docs/T3ILALIG40ATAAAAAAAAAA==/",
        "_etag": "\"01008a9e-0000-0000-0000-5b7d56dc0000\"",
        "_attachments": "attachments/",
        "_ts": 1534940892
    }
    ```

10. In the Azure portal, go to the Azure Function App host, **20777a-func-\<*your name\>-\<the day*\>**.
11. Click the **ProcessDeliveries** function, and then click **Logs** at the bottom of the page. You should see log messages similat to thos below appearing as the documents are added to the **Deliveries** collection and are processed by the function.

    ```Text
    2018-10-11T09:56:22.850 [Information] Executing 'Functions.ProcessDelivieries' (Reason='New changes on collection Deliveries at 2018-10-11T09:56:22.8492213Z', Id=0b26834f-24a3-4878-99e4-56ae95a4f93c)
    2018-10-11T09:56:22.899 [Information] Product: BK-M18S-48, Volume: 83, Time: 10/11/2018 9:56:15 AM
    2018-10-11T09:56:24.257 [Information] product is Mountain-500 Silver, 48
    2018-10-11T09:56:24.380 [Information] Document replaced. Status code is OK
    2018-10-11T09:56:24.380 [Information] Executed 'Functions.ProcessDelivieries' (Succeeded, Id=0b26834f-24a3-4878-99e4-56ae95a4f93c)
    ```

### Task 7: Clean up the lab environment

1. Stop the **ProductScanner** app and then close Visual Studio.
2. In the Azure portal, in the left pane, click **Resource groups**.
3. In the **Resource groups** blade, click the **20777Mod07** resource group.
4. In the **20777Mod07** blade, click **Delete resource group**.
5. In the **Are you sure you want to delete "20777Mod07"?** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **20777Mod07**, and then click **Delete**.

---
© 2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
