# Module 4: Optimizing and Monitoring Performance

- [Module 4: Optimizing and Monitoring Performance](#module-4-optimizing-and-monitoring-performance)
  - [Lab: Tuning and Monitoring Performance in a Cosmos DB Application](#lab-tuning-and-monitoring-performance-in-a-cosmos-db-application)
  - [Exercise 1: Examining Cosmos DB execution statistics](#exercise-1-examining-cosmos-db-execution-statistics)
    - [Task 1: Prepare the Environment](#task-1-prepare-the-environment)
    - [Task 2: Examine the data in the collection and assess the performance of common queries](#task-2-examine-the-data-in-the-collection-and-assess-the-performance-of-common-queries)
    - [Task 3: Gather performance statistics in an application](#task-3-gather-performance-statistics-in-an-application)
  - [Exercise 2: Monitoring Performance using the Azure Portal](#exercise-2-monitoring-performance-using-the-azure-portal)
    - [Task 1: Create and populate collections](#task-1-create-and-populate-collections)
    - [Task 2: Run a workload that tests the performance of each partitioning scheme](#task-2-run-a-workload-that-tests-the-performance-of-each-partitioning-scheme)
    - [Task 3: Assess the results](#task-3-assess-the-results)
  - [Exercise 3: Assessing the Impact of Consistency Levels](#exercise-3-assessing-the-impact-of-consistency-levels)
    - [Task 1: Create the AddPriceHistoryToDocument trigger](#task-1-create-the-addpricehistorytodocument-trigger)
    - [Task 2: Test the trigger using different consistency levels](#task-2-test-the-trigger-using-different-consistency-levels)
    - [Task 3: Replicate the database across regions, and retest the trigger](#task-3-replicate-the-database-across-regions-and-retest-the-trigger)
  - [Exercise 4: Investigating the Effects of Triggers on Performance](#exercise-4-investigating-the-effects-of-triggers-on-performance)
    - [Task 1: Implement the CreatePriceHistoryDocument trigger](#task-1-implement-the-createpricehistorydocument-trigger)
    - [Task 2: Compare the performance of using the trigger to performing the same operation in a client application](#task-2-compare-the-performance-of-using-the-trigger-to-performing-the-same-operation-in-a-client-application)
    - [Task 3: Cleanup the lab environment](#task-3-cleanup-the-lab-environment)

## Lab: Tuning and Monitoring Performance in a Cosmos DB Application

## Exercise 1: Examining Cosmos DB execution statistics

### Task 1: Prepare the Environment

1. Ensure that the **MT17B-WS2016-NAT** and **20777A-LON-DEV** virtual machines are running, and then log on to **20777A-LON-DEV** as **LON-DEV\\Administrator** with the password **Pa55w.rd**.
2. In File Explorer, navigate to **E:\\Labfiles\\Lab04\\Starter**, right-click **full-cosmos-setup.ps1.txt**, and then click **Edit**.
3. In Notepad, on line 2, replace the text **\<*your initials*\>** with your initials, and replace **\<*day*\>** with the day of the month. 
4. On line 3, change the **resourceGroupLocation** variable to reference your nearest location.
5. On the **File** menu, click **Save**.
6. On the **File** menu, click **Exit**.
7. In File Explorer, in the **E:\\Labfiles\\Lab04\\Starter** folder, right-click **Setup.cmd**, and then click **Run as administrator**.
8. At the **Security warning** messages, type **R**, and then press Enter.
9.  When prompted, enter your Azure credentials.
10. When the script has completed, press any key.
11. On the toolbar, click **Internet Explorer**.
12. In Internet Explorer, go to **http://portal.azure.com**, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
13. In the Azure portal, in the left panel, click **All resources**, and then click **20777-mod4-sql-\<*your initials and day*\>**.
14. On the **20777-mod7-sql-\<*your initials and day*\>** blade, under **Settings**, click **Keys**.
15. Make a note of the **URI**, and **PRIMARY KEY** values.

### Task 2: Examine the data in the collection and assess the performance of common queries

1. On the **20777-mod4-sql-\<*your initials and day*\>** blade, click **Data Explorer**.
2. In the **SQL API** pane, expand **Adventure-Works**, expand **Data**, and then click **Documents**.
3. In the toolbar, click **Settings**.
4. On the **Settings** blade, under **Page options**, click **Custom**.
5. In the **Query results per page** box, type **600**, and then click **Apply**.
6. In the **SQL API** pane, click **New SQL Query**.
7. On the **Query 1** tab, enter the following query, and then click **Execute Query**. This query should return 4 documents; one for each type of sock that Adventure-Works sells.

    ```SQL
    SELECT * FROM c WHERE c.subcategory = "Socks" AND c.doctype = "Product"
    ```

8. Change the query as shown below, and then click **Execute Query**. This query finds all documents with **doctype** set to **ProductHistory**. It should return 108 documents, each one showing the price of product 709 (one of the "sock" products) at some point in the past. Notice that to get the price history in sequence, you must order the data.

    ```SQL
    SELECT * FROM c WHERE c.subcategory = "Socks" AND c.doctype = "ProductHistory" AND c.productid = "709" ORDER BY c.pricedate
    ```

9.  Click **Query Stats**, and make a note of the **Request Charge** showing the number of RUs required to perform this query.
10. Change the query as shown below, and then click **Execute Query**.

    ```SQL
    SELECT * FROM c WHERE c.subcategory = "Socks" AND c.doctype = "ProductWithPriceHistory" AND c.productid = "709"
    ```

11. Click **Results**. This time you should see only a single document containing the product details and its full price history. Notice that the items in the **pricehistory** array are already stored in order, so you don't need to sort them as part of the query.
12. Click **Query Stats**, and compare the **Request Charge** to that of the previous query. It should be significantly lower.
13. Change the query as shown below, and then click **Execute Query**.

    ```SQL
    SELECT c.productcategory, c.productname, c.productnumber, c.color, c.listprice, c.size, c.quantityinstock FROM c WHERE c.subcategory = "Socks" AND c.doctype = "ProductWithPriceHistory" AND c.productid = "709"
    ```

14. Click **Results**. This query should return the details of a single product from a **ProductWithPriceHistory** document (the price history is not included in the output).
15. Click **Query Stats**, and note the **Request Charge**.
16. Change the query as shown below, and then click **Execute Query**

    ```SQL
    SELECT c.productcategory, c.productname, c.productnumber, c.color, c.listprice, c.size, c.quantityinstock FROM c WHERE c.subcategory = "Socks" AND c.doctype = "Product" AND c.productid = "709"
    ```

17. Click **Results**. This query should return the same data as before, but using the **Product** document instead.
18. Click **Query Stats**, and compare the **Request Charge** to that of the previous query. It should be less than the previous query. This is because the amount of I/O that Cosmos DB had to perform to fetch the document is much lower due to the document being smaller.

### Task 3: Gather performance statistics in an application

1. On the Start menu, click **Visual Studio 2017**.
2. On the **File** menu, point to **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, in the File name box, type **E:\\Labfiles\\Lab04\\Starter\\Exercise01\\ProductCatalogPerformance\\ProductCatalogPerformance.sln**, and then click **Open**. This app enables you to perform a number of queries for product information, to test the performance of the different ways of structuring the product documents.
4. In the **Security Warning for ProductCatalogPerformance** dialog box, clear the **Ask me for every project in this solution** check box, and then click **OK**.
5. In Solution Explorer, click **Program.cs**.
6. On the **Program.cs** tab, locate the **FindProductDocumentByID** method. This method retrieves a product by using the **ReadDocumentAsync** method. **ReadDocumentAsync** requires you to specify the document Uri, and the partition key for the partition containing the document. It provides the fastest access to a document in a collection.
7. In the **FindProductDocumentByID** method, under the **// TODO write out request change and request latency** comment, add the following code. This code prints the request charge and time taken to the console:

    ```Csharp
    Console.WriteLine($" Request charge: {documentResponse.RequestCharge} RUs \n Request Latency: {documentResponse.RequestLatency} ms");
    ```

8. Locate the **RunAQuery** method. This method prompts the user to enter an abitrary query and then runs it (no checks are performed to ensure tha the query is valid). The query is performed with the **EnableCrossPartitionQuery** option set to true.
9. Locate the **FindProductDocumentByQueryId** method. This method retrieves a document by performing an SQL query. Notice that the FeedOptions used by this query enable cross partition searches, and will scan every partition in the database to find matching documents.
10. Locate the **FindProductDocumentByQueryIdAndPartition** method. This method is similar to the the **FindProductDocumentByQueryId** method, except that th query it runs expects you to provide a partition key, and the query will only search the specified partition.
11. Find the **OutputProductResult** method. This method runs a specified query and displays the results.
12. In the **OutputProductResult** method, under the **// TODO create local variables to aggregate metrics** comment, add the following code to initialize the local variables that you will populate with performance information:

    ```CSharp
    double requestCharge = 0;
    double indexLookupTime = 0;
    double indexHitRatio = 0;
    double documentLoadTime = 0;
    double runtimeExecutionTimes = 0;
    ```
13. In the same method, under the **// TODO aggregate metrics** comment, add the following code to aggregate the metrics to the local variables that you set up in the previous step:

    ```Csharp
    requestCharge += queryResponse.RequestCharge;
    if (queryResponse.QueryMetrics.Count() > 0)
    {
        var queryMetrics = queryResponse.QueryMetrics.First().Value;
        indexLookupTime += queryMetrics.QueryEngineTimes.IndexLookupTime.TotalMilliseconds;
        indexHitRatio = queryMetrics.IndexHitRatio;
        documentLoadTime += queryMetrics.QueryEngineTimes.DocumentLoadTime.TotalMilliseconds;
        runtimeExecutionTimes += queryMetrics.QueryEngineTimes.RuntimeExecutionTimes.TotalTime.TotalMilliseconds;
    }
    ```

14. Under the **// TODO Display metrics** comment, add the following code to print the metrics to the console:

    ```Csharp
    Console.WriteLine($"\n Request charge: {requestCharge} RUs\n Index LookUp Time: {indexLookupTime} ms\n Index Hit Ratio: {indexHitRatio * 100}%\n Document Load Time: {documentLoadTime} ms\n Runtime Execution Time: {runtimeExecutionTimes} ms\n");
    ```

15. In Solution Explorer, click **App.config**.
16. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod4-sql-\<*your initials and day*\>** Cosmos DB account.
17. Press F5 to build and run the application. 
18. At the prompt, type **A** to retrieve a document by id/subcategory using the **ReadDocumentAsync** method.
19. At the **Enter document ID** prompt, type **709**, and then press Enter.
20. At the **Enter partition key (Subcategory)** prompt, type **Socks**, and then press Enter.
21. Note the value of the **Request charge** for the query.
22. At the prompt, type **B** to retrieve a document with a query by id only (a cross partition query).
23. At the **Enter product id** prompt, type **709**, and then press Enter.
24. At the **Enter subcategory** prompt, type **Socks**, and then press Enter.
25. Note that the **Request charge** is much higher than that of the partitioned query run by using the **ReadDocumentAsync** method.
26. At the prompt, type **C** to retrieve a document with a query by id and partition key.
27. At the **Enter product id** prompt, type **709**, and then press Enter.
28. At the **Enter partition key (Subcategory)** prompt, type **Socks**, and then press Enter.
29. Note that the **Request charge** is much lower than the previous example, but not as low as that of the **ReadDocumentAsync** method.
30. At the prompt, type **R** to invoke the **RunAQuery** method.
31. At the **Type your query** prompt, type the following query, and then press Enter to compare the effect of embedding the price history information in the product documents with holding the price history in individual documents on queries that read product information:

    ```SQL
    SELECT c.id, c.productcategory, c.productname, c.productnumber, c.listprice FROM c where c.doctype = 'Product'
    ```
32. Note the metrics.
33. At the prompt, type **R**.
34. At the **Type your query** prompt, type the following query, and then press Enter:

    ```SQL
    SELECT c.id, c.productcategory, c.productname, c.productnumber, c.listprice FROM c where c.doctype = 'ProductWithPriceHistory'
    ```

35. Compare the metrics from both queries. Observe that the larger documents (the second query) have a higher RU cost to return the same information (~20-25%). The **Document Load Time** has more than doubled.

    **Question**: What conclusions do you draw from these results?

    **Answer**: If many applications use the product catalog information from Cosmos DB, the performance of applications that do not need to access price history data might be negatively affected if the price history data is stored in the product documents.

36. At the prompt, type **R**.
37. At the **Type your query** prompt, type the following query, and then press Enter to compare the performance of retrieving price history information:

    ```SQL
    SELECT c.productid, c.listprice, c.pricedate from c where c.doctype = 'ProductHistory'
    ```

38. Note the metrics.
39. At the prompt, type **R**.
40. At the **Type your query** prompt, type the following query, and then press Enter:

    ```SQL
    SELECT x.productid, x.listprice, x.pricedate FROM c JOIN x IN c.pricehistory where c.doctype = 'ProductWithPriceHistory'
    ```

41. Compare the metrics from both queries. Observe that the RU cost of the second query is ~30% lower. The **Document Load Time** and **Runtime Execution Time** are also significantly lower.

    **Question**: What conclusions do you draw from these results? What changes might you implement to improve the performance of retrieving price history data?

    **Answer**: For very small documents like these price history records, the SQL API is less efficient at returning many documents than it is at returning the same information stored as subdocuments of a larger document.
    One possible performance enhancement is to group all the price history information for a product into a wrapper document at product level.

42. At the prompt, type **X** to exit the application.
43. In Visual Studio 2017, on the **Program.cs** tab, locate the **CreateProductHistories** method.
44. Replace the existing code in this method with that shown below. This code creates seperate product history documents from the prices recorded in the price history array of the **ProductWithPriceHistory** documents.

    ```CSharp
    private async Task CreateProductHistories()
    {
        SqlQuerySpec querySpec = new SqlQuerySpec()
        {
            QueryText = "SELECT * FROM c WHERE c.doctype = 'ProductWithPriceHistory'"
        };

        FeedOptions queryOptions = new FeedOptions { MaxItemCount = -1, EnableCrossPartitionQuery = true, PopulateQueryMetrics = this.enableLogging };
        var productWithHistoryQuery = client.CreateDocumentQuery<ProductWithHistory>(UriFactory.CreateDocumentCollectionUri(this.database, this.collection), querySpec, queryOptions).AsDocumentQuery();

        while (productWithHistoryQuery.HasMoreResults)
        {
            var queryResponse = (productWithHistoryQuery.ExecuteNextAsync()).Result;
            foreach (var queryDoc in queryResponse)
            {
                dynamic productHistories = new System.Dynamic.ExpandoObject();
                productHistories.productid = queryDoc.productid;
                productHistories.pricehistory = queryDoc.pricehistory;
                productHistories.doctype = "ProductHistories";
                productHistories.subcategory = queryDoc.subcategory;

                var createResponse = await client.CreateDocumentAsync(UriFactory.CreateDocumentCollectionUri(this.database, this.collection), productHistories);
            }
        }
    }
    ```

45. Press F5 to build and run the application. 
46. At the prompt, type **D** to create the **ProductHistories** records for each product in the collection. This operation will take a little time.
47. At the prompt, type **H** to prevent results being displayed.
48. At the prompt, type **R**.
49. At the **Type your query** prompt, type the following query, and then press Enter to compare the performance of retrieving price history information:

    ```SQL
    SELECT x.productid, x.listprice, x.pricedate FROM c JOIN x IN c.pricehistory WHERE c.doctype = 'ProductHistories'
    ```

50. Compare the metrics with the product history query metrics that you recorded earlier in the exercise. Notice that the **ProductHistories** query has similar metrics to the **ProductWithPriceHistory** query. This approach gives you good performance for both price history and standard product catalog queries.
51. At the prompt, type **X** to close the app, and then close Visual Studio 2017.


## Exercise 2: Monitoring Performance using the Azure Portal

### Task 1: Create and populate collections

1. In the Azure portal, in the left panel, click **All resources**, and then click **20777-mod4-sql-\<*your initials and day*\>**.
2. On the **20777-mod4-sql-\<*your initials and day*\>** blade, click **Data Explorer**.
3. In the **SQL API** pane, expand **Adventure-Works**, right-click **Data**, and then click **Delete Collection**.
4. On the **Delete Collection** blade, in the **Confirm by typing the collection id** box, type **Data**, and then click **OK**.
5. Using File Explorer, go to **E:\\Labfiles\\Lab04\\Starter**, right-click **create-database-partitioned-by-subcategory.ps1**, and then click **Edit**.
6. In the **Open File - Security Warning** dialog box, click **Open**.
7. In PowerShell ISE, in the code window, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
8.  Press F5 to run the script.
9.  In the **Windows PowerShell ISE** message box, click **OK**.
10. Wait while the script creates the **Data** collection and uploads the data. There should be **34344** documents.
11. In PowerShell ISE, on the **File** menu, click **Open**.
12. In the **Open** dialog box, in the **E:\\Labfiles\\Lab04\\Starter** folder, click **create-database-partitioned-by-productid.ps1**, and then click **Open**.
13. In PowerShell ISE, in the code window, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
14. Press F5 to run the script.
15. In the **Windows PowerShell ISE** message box, click **OK**.
16. Wait while the script creates the **Data2** collection and uploads the data. Again, there should be **34344** documents.
17. In PowerShell ISE, on the **File** menu, click **Open**.
18. In the **Open** dialog box, in the **E:\\Labfiles\\Lab04\\Starter** folder, click **create-database-partitioned-by-doctype.ps1**, and then click **Open**.
19. In PowerShell ISE, in the code window, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
20. Press F5 to run the script.
21. In the **Windows PowerShell ISE** message box, click **OK**.
22. Wait while the script creates the **Data3** collection and uploads the data. This script should also load **34344** documents.
23. In PowerShell ISE, on the **File** menu, click **Open**.
24. In the **Open** dialog box, in the **E:\\Labfiles\\Lab04\\Starter** folder, click **create-database-not-partitioned.ps1**, and then click **Open**.
25. In PowerShell ISE, in the code window, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
26. Press F5 to run the script.
27. In the **Windows PowerShell ISE** message box, click **OK**.
28. Wait while the script creates the **Data4** collection and uploads the data. This script should load **34344** documents.
29. In Internet Explorer, in the left panel, click **All resources**, and then click **20777-mod4-sql-\<*your initials and day*\>**.
30. On the **20777-mod4-sql-\<*your initials and day*\>** blade, click **Data Explorer**, and then expand **Adventure-Works**.
31. Verify that the database contains four collections, named **Data**, **Data2**, **Data3**, and **Data4**. These are the collections created by the PowerShell scripts. **Data** is the collection partitioned by **subcategory**, **Data2** is partitioned by **productid**, **Data3** is partitioned by **doctype**, and **Data4** is not partitioned.
32. Expand **Data**, and then click **Scale & Settings**. Verify that the **Throughput** is set to **5000 RU/s**.
33. Expand **Data2**, and then click **Scale & Settings**. Verify that the **Throughput** is set to **5000 RU/s**.
34. Expand **Data3**, and then click **Scale & Settings**. Verify that the **Throughput** is set to **5000 RU/s**.
35. Expand **Data4**, and then click **Scale & Settings**. Verify that the **Throughput** is set to **5000 RU/s**.

    > **Note:** For the partitioned collections, **Data**, **Data2**, and **Data3**, the throughput will be shared across five partition key ranges; each partition key range will be allocated **1000 RU/s**.

### Task 2: Run a workload that tests the performance of each partitioning scheme

1. On the Start menu, click **Visual Studio 2017**.
2. On the **File** menu, point to **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab04\\Starter\\Exercise02\\QueryPerformanceTester**, and then double-click **QueryPerformanceTester.sln**. This app simulates multiple users performing a number of queries against each of the collections.
4. In the **Security Warning for ProductCatalogPerformance** dialog box, clear the **Ask me for every project in this solution** check box, and then click **OK**.
5. In Solution Explorer, click **Program.cs**.
6. On the **Program.cs** tab, locate the **FindProductDocuments** method. This method runs the following query to find the documents for a product given the subcategory and productid. Note that both of these parameters are necessary to satisfy the requirements for the collections partitioned by subcategory and productid; it is important to run the same query across all collections to get a true comparison of performance.

    ```SQL
    SELECT * FROM c WHERE c.subcategory = @subcategory AND c.productid = @productid AND (c.doctype = 'Product' OR c.doctype = 'ProductHistory')
    ```

7. Locate the **DoWork** method. This method connects to the Cosmos DB database, and the invokes the **FindProductDocuments** method repeatedly, specify each of the collections in turn. The code that does this is highlighted below. The boolean parameter passed to the **FindProductDocuments** method indicates whether the method should enable cross-partition queries. This parameter is only set to true for the query that retrieves data from the collection partitioned by doctype (product and product price history documents are stored in different partitions).

    ```CSharp
    int numIterations = int.Parse(ConfigurationManager.AppSettings["NumIterations"]);
    for (int i = 0; i < numIterations; i++)
    {
        Console.WriteLine($"Worker{workerNum}, Iteration {i}");
        FindProductDocuments(collectionPartitionedBySubcategory, false);
        FindProductDocuments(collectionPartitionedByProductID, false);
        FindProductDocuments(collectionPartitionedByDocType, true);
        FindProductDocuments(collectionNotPartitioned, false);
    }
    ```

8. In Solution Explorer, click **App.config**.
9. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
10. Press F5 to build and run the application. The application will take 15-20 minutes to complete. Make a note of the time you started running the application, and then when the application closes.


### Task 3: Assess the results

1. In the Azure portal, in the left panel, click **All resources**, and then click **20777-mod4-sql-\<*your initials and day*\>**.
2. On the **20777-mod4-sql-\<*your initials and day*\>** blade, under **Monitoring**, click **Metrics**.
3. On the **Metrics** blade, on the **Storage** tab, in the **Database(s)** drop-down list, click **Adventure-Works**.
4. In the **Collection(s)** drop-down list, click **Data**. This is the collection partitioned by subcategory.
5. In the **Data + Index storage consumed by top partition keys** graph, observe that there is significant skew in the partition sizes.
6. Click the biggest partition; the partition key should be **Road Bikes**.
7. Click the biggest partition to clear it, and then click the second biggest partition; the partition key should be **Mountain Frames**.

    **Question**: What is the principal drawback of having partitions skewed in this way, and what has caused it in this case?

    **Answer**: Skewed partitions indicate an imbalance of data across the different partition key values, and can lead to contention occurring if the data for a specific partition is accessed much more frequently than others. In this collection, there are clearly many more documents relating to products in the **Mountain Bike** subcategory than there are for other subcategories. This skew might change over time as other products are added in other subcategories. However, you should monitor this skew, and be prepared to repartition the data if it becomes excessive and shows no sign of abating.

8.  In the **Data + Index storage consumed per partition key range** graph, click the biggest partition range.

    **Question**: The data for which partitions is stored in this partition key range?

    **Answer**: This partition key range contains data for **Mountain Frames**, **Road Frames**, and **Touring Frames** (it might contain other partitions as well; the table next to the graph only lists the top three partition keys).

9.  In the **Collection(s)** drop-down list, click **Data2**. This is the collection partitioned by productid.
10. In the **Data + Index storage consumed by top partition keys** graph, notice that this partitioning scheme has distributed the data more evenly across the logical partitions.
11. In the **Data + Index storage consumed per partition key range** graph, again notice that the size of the partition key ranges are more even than the previous scheme.
12. In the **Collection(s)** drop-down list, click **Data3**. This is the collection partitioned by doctype.
13. In the **Data + Index storage consumed by top partition keys** graph, note that this time, the partitons are very skewed, with the vast majority of documents in the **ProductHistory** partition. Remember from exercise 1 that a new  **ProductHistory** document is created every time a product has a price change, so this skew is unlikely to change and will probably only get worse.
14. In the **Collection(s)** drop-down list, click **Data4**. This is the non-partitioned collection.
15. In the **Data + Index storage consumed by top partition keys** graph, notice that data is held in a single unamed partition.
16. On the **Throughput** tab, in the **Database(s)** drop-down list, click **Adventure-Works**.
17. In the **Collection(s)** drop-down list, click **Data**.
18. In the timeline toolbar, click **Custom**.
19. Set the **Start**, and **End** times to the time you noted earlier when running the application, and then click **Apply**.
20. In the **Number of requests (aggregated over 5 minute interval)** graph, note the average number of requests reported per minute while the app was running; it should be just over 1K.
21. In the **Max consumed RU/s per partition key range** graph, note the average RU/s for each partition key range. There may be some spikes, but it will typically hover around 500 (0.5K) most of the time.
22. Where you do get any spikes, click the spike on the graph. The **Max consumed RU/s by each partition key range** chart below the graph will show how the partitions were being accessed at that point in time.
23. In the **Collection(s)** drop-down list, click **Data2**.
24. Verify that the **Number of requests (aggregated over 5 minute interval)** graph shows a similar number of requests reported per minute to this partition while the app was running; it should be just over 1K.
25. In the **Max consumed RU/s per partition key range** graph, you should see that the more even distribution provided by this partitioning scheme has prevented any single partition from exceeding its throughput resources. The average figure should be somewhere around 0.4K RU/s per partition key range.
26. In the **Collection(s)** drop-down list, click **Data3**.
27. Verify that the **Number of requests (aggregated over 5 minute interval)** graph shows a similar number of requests reported per minute to this partition while the app was running; it should be just over 2K.
28. In the **Max consumed RU/s per partition key range** graph, this time you should see that this partitioning scheme consumes far more resources than the previous one (probably close to double). The average figure should be somewhere around 0.8K RU/s per partition key range.
29. Click one of the peaks, and then examine the **Max consumed RU/s by each partition key range** chart.

    **Question**: What do you notice about this chart?

    **Answer**: This chart indicates that the collection is currently only using two partition key ranges, and one is far more densely populated than the other. Of the the throughput allocated to the collection, 5000 RU/s, only 2000 RU/s wll be used; the remaining 3000 RU/s are currently a costly waste.

30. In the **Collection(s)** drop-down list, click **Data4**.
31. In the **Max consumed RU/s per partition key range** graph, this time you should see that this partitioning scheme consumes even more resources than the previous one per partition key range (somewhere around 1K).

    **Question**: Given the findings displayed by the **Metrics** blade, which partitioning scheme appears to be the most optimal for storing product and product price history documents?

    **Answer**. In this lab, the most effciient strategy appears to be that implemented by the **Data2** collection. This collection partitions documents by product id. However, you should also consider that the Adventure-Works application uses other types of documents (shopping carts, orders, and so on), and this partitioning approach might not be the most suitable for these types of documents. If a collection stores many different types of documents, you need to strike a balance between the partitioning requirements of these different types.  

32. Close Visual Studio 2017.

## Exercise 3: Assessing the Impact of Consistency Levels

### Task 1: Create the AddPriceHistoryToDocument trigger

> **Note:** The JavaScript code used in this task can be found in the file **E:\\Labfiles\\Lab04\\\Solution\\Exercise03\\AddPriceHistoryToDocument.txt**.

1. On the **20777-mod4-sql-\<*your initials and day*\>** blade, click **Data Explorer**.
2. In the **Message from webpage** dialog box, click **OK**.
3. In the **SQL API** pane, expand **Adventure-Works**, right-click **Data2**, and then click **New Trigger**.
4. On the **New Trigger 1** blade, in the **Trigger Id** box, type **AddPriceHistoryToDocument**.
5. Ensure the **Trigger Type** is set to **Pre**.
6. Ensure the **Trigger Operation** is set to **All**.
7. In the **Trigger Body** box, change the code as shown below, and then click **Save**:

    ```JavaScript
    function trigger(){
        var context = getContext();  
        var request = context.getRequest();
        var collection = context.getCollection();  
        var documentBeingUpdated = request.getBody();

        // If the operation being performed is not an upsert, then ignore this trigger
        if (request.getOperationType() != "Upsert") {
            return;
        }

        // Verify the type of the document - should be a ProductWithPriceHistory
        if (!("doctype" in documentBeingUpdated) && documentBeingUpdated.doctype != "ProductWithPriceHistory") {
            throw new Error("Wrong type of document");
        }

        // Create a pricehistory subdocument using the data from the document triggering the operation
        var newPriceHistorySubDoc =
        {
            productid: documentBeingUpdated.productid,
            listprice: documentBeingUpdated.listprice,
            pricedate: new Date().toISOString().slice(0,10)
        };

        // Add the pricehistory subdocument to start of the pricehistory array
        documentBeingUpdated.pricehistory.push(newPriceHistorySubDoc);

        // Save the changes
        request.setBody(documentBeingUpdated);
    }
    ```

### Task 2: Test the trigger using different consistency levels

1. On the **20777-mod4-sql-\<*your initials and day*\>** blade, under **Settings**, click **Default consistency**.
2. Verify that the default consistency level is **BOUNDED STALENESS**. Applications can override this consistency level to lower it, but cannot increase it to **STRONG**.
3. On the Start menu, click **Visual Studio 2017**.
4.  On the **File** menu, point to **Open**, and then click **Project/Solution**.
5.  In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab04\\Starter\\Exercise03\\PriceChangeTester**, and then double-click **PriceChangeTester.sln**. 
6.  In the **Security Warning for ProductCatalogPerformance** dialog box, clear the **Ask me for every project in this solution** check box, and then click **OK**.
7.  In Solution Explorer, double-click **Program.cs**.
8.  On the **Program.cs** tab, in the **Worker** class, locate the **DoWork** method. This method connects to the database, reads the document for a specified product, changes the price (it adds 0.5), and then saves it back to the collection, firing the **AddPriceHistoryToDocument** trigger as it does so. Note that the update is protected by using the **ETag** to prevent any concurrent changes from being lost, as follows. If a concurrent change is detected, the app throws an exception which is caught and reported by the handler at the end of the method.

    ```CSharp
    // Save the doc back to the collection
    options = new RequestOptions
    {
        AccessCondition = new AccessCondition
        {
            Condition = document.ETag,
            Type = AccessConditionType.IfMatch
        },
        PreTriggerInclude = new List<string>{ "AddPriceHistoryToDocument" }
    };

    var updateResponse = await client.UpsertDocumentAsync(
        UriFactory.CreateDocumentCollectionUri(this.database, this.collection), document, options);
    Console.WriteLine("Document updated");
    ```

9.  In the **Program** class, locate the **Main** method. This method prompts the user for a product id, and then runs the **DoWork** method of a **Worker** object 100 times in quick succession. These runs are all performed sequentially rather than concurrently; only one update to the specified product document should occur at a time.

    ```CSharp
    static void Main(string[] args)
    {
        Console.WriteLine("Enter product ID");
        string id = Console.ReadLine();

        Worker worker = new Worker();
        for (int i = 0; i < 100; i++)
        {
            worker.DoWork(id).Wait();
        }

        Console.WriteLine("Press Enter to finish");
        Console.ReadLine();
    }
    ```

10. In Solution Explorer, click **App.config**.
11. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
12. Press F5 to build and run the application.
13. At the **Enter product ID** prompt, type **717**, and then press Enter. This will update the price of product 717. The app displays the complete document each time it is updated, and you should see the new price being appended to the array at the end of the document.
14. When the application has completed, press Enter to close it.
15. In Vsual Studio 2017, on the **Program.cs** tab, in the **DoWork** method, locate the following statement which creates the client object that connects to the database, notice that it specified a consistency level of **BoundedStaleness**:

    ```CSharp
    // Connect to the Cosmos DB account
    this.client = new DocumentClient(new Uri(endpointUrl), primaryKey, null, ConsistencyLevel.BoundedStaleness);
    ```

16. Change the consistency level to **ConsistencyLevel.Eventual**.
17. Press F5 to build and run the application again.
18. At the **Enter product ID** prompt, type **718**, and then press Enter.
19. Verify that the application operates as before, except that it is modifying the price of a different product.
20. When the application has completed, press Enter to close it.
21. Repeat steps 15 to 20, using the following consistency levels:
    - **ConsistencyLevel.Session** using product ID **719**
    - **ConsistencyLevel.ConsistentPrefix** using product ID **720**

### Task 3: Replicate the database across regions, and retest the trigger

1. In Internet Explorer, on the **Default consistency** blade, verify that it is still set to **BOUNDED STALENESS**.
2. In the **Maximum Lag (Time)** section, in the **Minutes** box, type **5**.
3. In the **Maximum Lag (Operations)** box, type **100000**, and then click **Save**, and wait for the changes to be applied.

   > **Note:** These changes are necessary as they are the minimum values supported by the bounded staleness consistency level when you replicate data across regions.

4. On the **20777-mod4-sql-\<*your initials and day*\>** blade, under **Settings**, click **Replicate data globally**.
5. On the **Replicate data globally** blade, click a location that is physically distant from your write region, and then click **Save**. For example, if your write region is **Central US**, click **Southeast Asia**. Wait while the replication is configured. This can take several minutes.
6. In Visual Studio 2017, on the **Program.cs** tab, in the **DoWork** method, change the consistency level used by the application to **BoundedStaleness** as follows:

    ``` CSharp
    // Connect to the Cosmos DB account
    this.client = new DocumentClient(new Uri(endpointUrl), primaryKey, null, ConsistencyLevel.BoundedStaleness);
    ```

7.  Press F5 to build and run the application.
8.  At the **Enter product ID** prompt, type **725**, and then press Enter.
9.  Verify that the app runs without reporting any exceptions, and then press Enter to close it.
10. In Visual Studio 2017, in the **Program.cs** file, in the **DoWork** method, change the consistency level used by the application to **Eventual** as follows:

    ``` CSharp
    // Connect to the Cosmos DB account
    this.client = new DocumentClient(new Uri(endpointUrl), primaryKey, null, ConsistencyLevel.Eventual);
    ```

11. Press F5 to build and run the application.
12. At the **Enter product ID** prompt, type **726**, and then press Enter. The application will start updating documents, but at some point may throw the following exception: 

    ```Text
    Message: {"Errors":["One of the specified pre-condition is not met"]}
    ActivityId: 5e601622-0365-4bc0-a6c7-62c47c2ae7c1, Request URI: /apps/3666e0f0-ed7d-4e2e-8558-f69907b4ff5d/services/6e89fcb4-cbba-4b28-81b0-4714995eb1e6/partitions/0b3a4866-d542-4610-b3f3-c814312956c0/replicas/131798616051453023p, RequestStats:
    RequestStartTime: 2018-08-29T10:56:36.7260805Z, Number of regions attempted: 0
    , SDK: Microsoft.Azure.Documents.Common/2.0.0.0, Windows/10.0.17134 documentdb-netcore-sdk/1.9.1
    ```

13. Press Enter and allow the app to continue. It may trigger more exceptions as the same problem recurs.
14. When the app has finished, press Enter to close the app.
15. Close Visual Studio 2017.

## Exercise 4: Investigating the Effects of Triggers on Performance

### Task 1: Implement the CreatePriceHistoryDocument trigger

> **Note:** The JavaScript code used in this task can be found in the file **E:\\Labfiles\\Lab04\\Solution\\Exercise04\\CreatePriceHistoryDocument.txt**.

1. In Internet Explorer, on the **20777-mod4-sql-\<*your initials and day*\>** blade, click **Data Explorer**.
2. In the **SQL API** pane, expand **Adventure-Works**, right-click **Data2**, and then click **New Trigger**.
3. On the **New Trigger 1** tab, in the **Trigger Id** box, type **CreatePriceHistoryDocument**.
4. Ensure the **Trigger Type** is set to **Pre**.
5. Ensure the **Trigger Operation** is set to **All**.
6. In the **Trigger Body** box, change the code as shown below, and then click **Save**:

    ```JavaScript
    function trigger(){
        var context = getContext();  
        var request = context.getRequest();
        var collection = context.getCollection();  
        var documentBeingUpdated = request.getBody();
 
        // If the operation being performed is not an upsert, then ignore this trigger
        if (request.getOperationType() != "Upsert") {
            return;
        }

        // Verify the type of the document - should be a Product
        if (!("doctype" in documentBeingUpdated) && documentBeingUpdated.doctype != "Product") {
            throw new Error("Wrong type of document");
        }

        // Create a ProductHistory document using the data from the document triggering the operation
        var priceDate = new Date().toISOString();
        var priceHistoryDoc =
        {
            id: documentBeingUpdated.productid + ":Price:" + priceDate,
            doctype: "ProductHistory",
            subcategory: documentBeingUpdated.subcategory,
            productid: documentBeingUpdated.productid,
            listprice: documentBeingUpdated.listprice,
            pricedate: priceDate
        }

        // Attempt to save the ProductHistory document
        var isAccepted = collection.createDocument(collection.getSelfLink(), priceHistoryDoc);

        // If the trigger is out of runtime, throw an error
        if (!isAccepted) {
            throw new Error('Unable to create price history document');
        }
    }
    ```

    > **Note:** For this exercise, the date is stored as a full ISO string, including the time down to the nearest milliscond. Previously, the document only stored the day, month, and year because it was assumed that a product would not change price more than once in a day. In this exercise, you are going to be performing a number of updates in quick succession, so just using the date without the time would result in multiple documents attempting to use the same document ID.


### Task 2: Compare the performance of using the trigger to performing the same operation in a client application

1. On the Start menu, click **Visual Studio 2017**.
2. On the **File** menu, point to **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab04\\Starter\\Exercise04\\PriceChangePerformanceTester**, and then double-click **PriceChangePerformanceTester.sln**.
4. In the **Security Warning for ProductCatalogPerformance** dialog box, clear the **Ask me for every project in this solution** check box, and then click **OK**.
5. In Solution Explorer, click **Program.cs**.
6. On the **Program.cs** tab, in the **Worker** class, locate the **DoWork** method. This method is very similar to that used by the **PriceChangeTester** app in exercise 3. The primary differences are that it takes an additional boolean parameter, **useTrigger**, that indicates whether the method should create the **ProductHIstory** document using the trigger you just created, or whether this task should be performed by the client.
7. Locate the **// Save the doc back to the collection** comment. This code calls either the **SaveDocumentUsingTrigger** or **SaveDocumentWithoutUsingTrigger** method, depending on the value of **useTrigger**.

    ```CSharp
    // Save the doc back to the collection
    double result = 0.0;
    if (useTrigger)
    {
        result = await SaveDocumentUsingTrigger(document);
    }
    else
    {
        result = await SaveDocumentWithoutUsingTrigger(document);
    }
    return result;
    ```

8.  Locate the **SaveDocumentUsingTrigger** method. This code is very similar to that used in the previous exercise, except that it runs the **CreatePriceHistoryDocument** trigger, and returns the request charge for performing this operation.

    ```CSharp
    // Save the document using the trigger to add the price change history document
    // Return the request charge
    private async Task<double> SaveDocumentUsingTrigger(Document document)
    {
        var options = new RequestOptions
        {
            AccessCondition = new AccessCondition
            {
                Condition = document.ETag,
                Type = AccessConditionType.IfMatch
            },
            PreTriggerInclude = new List<string> { "CreatePriceHistoryDocument" }
        };

        var updateResponse = await client.UpsertDocumentAsync(
            UriFactory.CreateDocumentCollectionUri(this.database, this.collection), document, options);
        Console.WriteLine("Document updated using trigger");
        return updateResponse.RequestCharge;
    }
    ```

9.  Locate the **SaveDocumentWithoutUsingTrigger** method. This code saves the **Product** document without firing the trigger. It then creates a **ProductHistory** document and inserts it into the database. The request charge for both operations is aggregated and returned.

    ```CSharp
    // Save the document without using the trigger
    // Create and save the price change history document manually
    // Return the request charge
    private async Task<double> SaveDocumentWithoutUsingTrigger(Document document)
    {
        var options = new RequestOptions
        {
            AccessCondition = new AccessCondition
            {
                Condition = document.ETag,
                Type = AccessConditionType.IfMatch
            }
        };

        var updateResponse = await client.UpsertDocumentAsync(
            UriFactory.CreateDocumentCollectionUri(this.database, this.collection), document, options);
        Console.WriteLine("Document updated without using trigger");

        var historyDocument = new Document();
        var priceDate = DateTime.Now.ToString("yyyy-MM-dd HH:mm:ss.fffffff");
        var productID = document.GetPropertyValue<string>("productid");
        historyDocument.SetPropertyValue("id", $"{productID}:Price:{priceDate}");
        historyDocument.SetPropertyValue("doctype", "ProductHistory");
        historyDocument.SetPropertyValue("subcategory", document.GetPropertyValue<string>("subcategory"));
        historyDocument.SetPropertyValue("productid", productID);
        historyDocument.SetPropertyValue("listprice", document.GetPropertyValue<string>("listprice"));
        historyDocument.SetPropertyValue("pricedate", priceDate);

        var insertResponse = await client.CreateDocumentAsync(
            UriFactory.CreateDocumentCollectionUri(this.database, this.collection), historyDocument);
        Console.WriteLine("History document added");

        return updateResponse.RequestCharge + insertResponse.RequestCharge;
    }
    ```

10. In the **Program** class, locate the **Main** method. This method prompts the user for a product id, and then iterates 100 times run the **DoWork** method both with and without invoking the trigger. The request charges are accumulated and displayed when the application completes. These figures should give you a comparison of the costs involved for performing the update using the trigger and by using code.

    ```CSharp
    static void Main(string[] args)
    {
        Console.WriteLine("Enter product ID");
        string id = Console.ReadLine();

        double requestChargeUsingTrigger = 0.0;
        double requestChargeWithoutUsingTrigger = 0.0;

        Worker worker = new Worker();
        for (int i = 0; i < 100; i++)
        {
            requestChargeUsingTrigger = worker.DoWork(id, true).Result;         // Use the trigger and get the response including the performance metrics
            requestChargeWithoutUsingTrigger = worker.DoWork(id, false).Result; // Don't use the trigger
        }

        Console.WriteLine($"Total request charge using trigger: {requestChargeUsingTrigger} RUs");
        Console.WriteLine($"Total request charge without using trigger: {requestChargeWithoutUsingTrigger} RUs");
        Console.WriteLine("Press Enter to finish");
        Console.ReadLine();
    }
    ```

11. In Solution Explorer, click **App.config**.
12. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-\<*your initials and day*\>** Cosmos DB account.
13. Press F5 to build and run the application.
14. At the **Enter product ID** prompt, type **730**. This will update the price of product 730 200 times; 100 times using the trigger and 100 times using client code.
15. When the app finishes, note the request charges with and without using the trigger, and then press Enter.
16. Repeat steps 13 to 15, using products **731**, **732**, and **733**. Each time note the request charges when the app completes.

    **Question**: What do you notice about the request charges for each run of the app?

    **Answer**. The request charge for performing the insert by using client code (not using the trigger) is the same each time. The request charge for using the trigger varies, and is usually (but not always) higher than not using the trigger. In this case, it would appear that using client code is the more efficient option. This is not a general rule though; you will have situations where the operation performed by a trigger is quicker than sending a seperate request from the client, and you should assess performance on a case by case basis.

17. Close Visual Studio 2017.

### Task 3: Cleanup the lab environment

1. In Internet Explorer, in the left panel, click **Resource groups**.
2. In the **Resource groups** blade, right-click **20777Mod04**, and then click **Delete resource group**.
3. In the **Are you sure you want to delete "20777Mod04"?** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **20777Mod04**, and then click **Delete**.

---
© 2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
